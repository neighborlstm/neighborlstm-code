{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Taxi Demand Prediction - Porto city - Portugal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Business/Real World Problem\n",
    "For a given Taxi stand in Porto, our goal is to <b>predict the number of pickups in that given stand</b>. Some location require more taxis at a particular time than other locations owing to the presence schools, hospitals, offices etc. The prediction result can be transferred to the taxi drivers via Smartphone app, and they can subsequently move to the stands where predicted pickups are high."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objectives & Constraints\n",
    "<b>Objectives:</b> Our objective is to predict the number of pickups as accurately as possible for each taxi stand in a 30-min interval. We have 63 taxi station in Porto city.<br><br>\n",
    "<b>Constraints:</b>\n",
    "* <b>Latency:</b> Given a location and current time of a taxi driver, as a taxi driver, he/she excepts to get the predicted pickups in his/her stand and the adjoining stands in few seconds. Hence, there is a medium latency requirement.<br><br>\n",
    "\n",
    "* <b>Interpretability:</b> As long as taxi driver gets good prediction result, he/she is not be much interested in the interpretability of the result. He/she is not much interested in why he/she is getting this result. Hence, there is a no interpretability required.<br><br>\n",
    "\n",
    "* <b>Relative Errors:</b> Symmetric Mean Absolute Percentage Error will be the relative error we will consider. Let say the predicted pickups for a particular station are 100, but actual pickups are 102, the percentage error will be 2% and Absolute error is 2. The taxi driver will be more interested in the percentage error than the absolute error. Let say in some stand the predicted pickups are 250, and if taxi driver knows that the relative error is 10% then he/she will consider the predicted result to be in the range of 225 to 275, which is considerable.<br><br>\n",
    "\n",
    "<b>Our goal is to reduce the percentage error as low as possible.</b> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Data Information\n",
    "Porto taxi dataset\n",
    "https://www.kaggle.com/c/pkdd-15-predict-taxi-service-trajectory-i/data\n",
    "We have used 01 Jun 2013 and 30 Jun 2014 data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table>\n",
    "    <tr>\n",
    "        <th>File-Name</th>\n",
    "        <th>File-Size</th>\n",
    "        <th>Number of Records (before)</th>\n",
    "        <th>Number of Records (after)</th>\n",
    "        <th>Number of Features</th>\n",
    "        </tr>\n",
    "    <tr>\n",
    "        <td> data_prcess.csv</td>\n",
    "        <td> 116MB</td>\n",
    "        <td> 1.710.670</td>\n",
    "        <td> 1.706.572</td>\n",
    "        <td> 8 </td>\n",
    "    </tr>\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.dataframe as dd\n",
    "import pandas as pd\n",
    "from pandas import read_csv\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import folium\n",
    "import gpxpy.geo\n",
    "from datetime import datetime\n",
    "import time\n",
    "import seaborn as sns\n",
    "import os\n",
    "import math\n",
    "import xgboost as xgb\n",
    "import matplotlib\n",
    "#from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "from sklearn.cluster import MiniBatchKMeans, KMeans\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.layers import LSTM\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter('ignore')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem Formulation: Time Series Forecasting\n",
    "\n",
    "Given a taxi stand and a 30-min interval, we have to predict pickups.\n",
    "* (a): Use taxi stand as the center of region\n",
    "* (b): Every stand in Porto Porto has to be broken up into 30-min  interval (set a parameter).<br>\n",
    "\n",
    "Now, every row pickup has longitude and latitude in it, and we will use this to define pickup locations. \n",
    "We already know, about the pickup at time 't', we will predict the pickup at time 't+1' in the same region. Hence, this problem can be thought of as a 'Time Series Prediction' problem. It is a special case of regression problems. In short, we will use the data at time 't' to predict for time 't+1'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance Metric\n",
    "* 1) Synmetric Mean Absolute Percentage Error(sMAPE)\n",
    "* 2) Mean Squared Error(MSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Get the dataframe which includes only required colums.\n",
    "# 2. Add time_bin (index of 10min intravel - use a parameter to which that trip belongs to).\n",
    "# 3. Group by data, based on 'pickup_cluster' and 'time_bin'\n",
    "startTime = datetime.now()\n",
    "frame_data = read_csv(\"/home/tai/data/data_process.csv\")\n",
    "\n",
    "print(\"PREPARATION OF July 2013-June 2014 DATA.\")\n",
    "print(\"-\"*35)\n",
    "\n",
    "print(\"Number of columns = \"+str(len(frame_data.columns)))\n",
    "print(\"-\"*35)\n",
    "\n",
    "print(\"Number of rows = \" + str(len(frame_data)))\n",
    "print(\"-\"*35)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coord = frame_data[[\"LATITUDE\", \"LONGITUDE\"]].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frame_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "minlat = frame_data['LATITUDE'].min()\n",
    "maxlat = frame_data['LATITUDE'].max()\n",
    "minlon = frame_data['LONGITUDE'].min()\n",
    "maxlon = frame_data['LONGITUDE'].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Min latitude: \"+str(minlat))\n",
    "print(\"Max latitude: \"+str(maxlat))\n",
    "print(\"Min LONGITUDE: \"+str(minlon))\n",
    "print(\"Max LONGITUDE: \"+str(maxlon))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coord=frame_data[['LATITUDE','LONGITUDE']].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coord"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(coord)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(coord)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#labels\n",
    "cluster_labels=frame_data['STAND'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "taxistand=read_csv(\"/home/tai/data/taxistand.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "taxistand"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plotting stand on the map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.flickr.com/places/info/2459115 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "centerOfRegions = taxistand[['Latitude','Longitude']].values\n",
    "noOfClusters = len(centerOfRegions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(centerOfRegions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "m = folium.Map(location = [41.161488, -8.628821], zoom_start=13)\n",
    "for i in range(noOfClusters):\n",
    "    folium.Marker([centerOfRegions[i][0], centerOfRegions[i][1]], popup = (str(np.round(centerOfRegions[i][0], 2))+\", \"+str(np.round(centerOfRegions[i][1], 2)))).add_to(m)\n",
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = folium.Map(location = [41.14961,-8.61099], zoom_start=13)\n",
    "for i in range(noOfClusters):\n",
    "    folium.Marker([centerOfRegions[i][0], centerOfRegions[i][1]], popup = taxistand.loc[i,'Descricao']).add_to(m)\n",
    "m\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plotting Regions in Porto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Porto_latitude_range = (41.0011, 41.2366)\n",
    "Porto_Longitude_range = (-8.7891, -7.8751)\n",
    "fig = plt.figure()\n",
    "ax = fig.add_axes([0,0,1,1])\n",
    "ax.scatter(x = frame_data['LONGITUDE'].values, y = frame_data['LATITUDE'].values, c = frame_data['STAND'].values, cmap = \"Paired\", s = 5)\n",
    "ax.set_xlim(-8.79, -7.87)\n",
    "ax.set_ylim(41.01, 41.24)\n",
    "ax.set_title(\"Regions in Porto City\")\n",
    "ax.set_xlabel('Longitude')\n",
    "ax.set_ylabel('Latitude')\n",
    "\n",
    "plt.show()\n",
    "#Longitude values vary from left to right i.e., horizontally\n",
    "#Latitude values vary from top to bottom means i.e., vertically"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Print number of items in taxi stand\n",
    "count_points = frame_data.groupby(['STAND'])['STAND'].count()\n",
    "point_count = count_points.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range (len(point_count)):\n",
    "    print(point_count[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Campanhã Railway Station is a 19th-century railway station in the civil parish of Campanhã, in the municipality of Porto, district of Porto. It is the busiest taxistand in Porto as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "busiest=point_count.argmax()\n",
    "taxistand.iloc[busiest]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "centerOfRegions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "centroids=centerOfRegions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Calculate new average distance from points to their centroids\n",
    "def k_mean_distance_new(data, cx, cy, i_centroid, cluster_labels):\n",
    "        distances = [gpxpy.geo.haversine_distance(cx,cy,x,y) for (x, y) in data[cluster_labels == i_centroid]]\n",
    "        return distances\n",
    "\n",
    "\n",
    "distances_new = []\n",
    "index=[]\n",
    "for i, (cx, cy) in enumerate(centroids):\n",
    "    if (i in count_points_new):\n",
    "        mean_distance_new = k_mean_distance_new(coord, cx, cy, i, cluster_labels)\n",
    "        distances_new.append(mean_distance_new)\n",
    "        index.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0,63):\n",
    "    plt.figure(figsize=(8,7))\n",
    "    plt.scatter(x = frame_data['LONGITUDE'][frame_data['STAND'].values==i].values, y = frame_data['LATITUDE'][frame_data['STAND'].values==i].values, cmap = \"Paired\", s = 5,alpha=0.7)\n",
    "    plt.xlim(-8.79, -7.87)\n",
    "    plt.ylim(41.01, 41.24)\n",
    "    plt.title(\"Stand \" + str(i+1))\n",
    "    plt.xlabel('Longitude')\n",
    "    plt.ylabel('Latitude')\n",
    "    plt.show()   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time Binning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1372636800 : 2013-07-01 00:00:00   (Equivalent unix time)\n",
    "# 1404172799 : 2014-06-30 00:00:00   (Equivalent unix time)\n",
    "\n",
    "def pickup_bins(dataframe, year, granularity):\n",
    "    pickupTime = dataframe[\"TIMESTAMP\"].values\n",
    "    unixTime = [1372636800, 1404172799]\n",
    "    unix_year = unixTime[year-2013]\n",
    "    time_bin = [int((i - unix_year)/(granularity*60)) for i in pickupTime]\n",
    "    dataframe[\"TIME_BIN\"] = np.array(time_bin)\n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "porto_data = pickup_bins(frame_data, 2013,30)\n",
    "print(\"Pickup time bins are assigned\")\n",
    "print(\"-\"*35)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "porto_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of time bin: \", str(len(np.unique(porto_data[\"TIME_BIN\"]))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "porto_data_timeBin_groupBy = porto_data[[\"STAND\", \"TIME_BIN\", \"CALL_TYPE\"]].groupby(by = [\"STAND\", \"TIME_BIN\"]).count()\n",
    "print(\"Stand and time bins are grouped.\")\n",
    "print(\"-\"*35)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "porto_data_timeBin_groupBy.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EXtracting more temporal features from the dataset\n",
    "\n",
    "For each trip we will extract the weekday and the hour of pickup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frame_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "#convert timestamp to datetime in YMDHMS column\n",
    "pickupTime = porto_data[\"TIMESTAMP\"].values\n",
    "YMDHMS = [datetime.fromtimestamp(i).strftime('%Y-%m-%d-%H-%M-%S') for i in pickupTime]\n",
    "porto_data[\"Pickup_YMDHMS\"] = np.array(YMDHMS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Exctract pickup hour\n",
    "pu_hour = [int(i.split('-')[3]) for i in YMDHMS]\n",
    "porto_data[\"Pickup_hour\"] = np.array(pu_hour)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#extract pickup weekday for each trip\n",
    "pu_weekday=[datetime.strptime(str(i),'%Y-%m-%d-%H-%M-%S').strftime('%A') for i in YMDHMS]\n",
    "porto_data[\"Pickup_weekday\"] = np.array(pu_weekday)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "porto_data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import create_engine\n",
    "\n",
    "porto_db = create_engine('sqlite:///porto_db.db')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "con = sqlite3.connect('C:\\\\Users\\\\Bahman\\\\Traffic Data - LUH\\\\Thesis Codes\\\\porto_db.db')\n",
    "cursor = con.cursor()\n",
    "cursor.execute(\"SELECT name FROM sqlite_master WHERE type='table';\")\n",
    "print(cursor.fetchall())\n",
    "con.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#porto_data.to_sql('taxidata_table_record', porto_db, if_exists='append') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ddd = pd.read_sql_query('SELECT * FROM taxidata_table_record ', porto_db)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pu = pd.read_sql_query('SELECT Pickup_hour AS time, count(*) AS PUcount \\\n",
    "                        FROM taxidata_table_record \\\n",
    "                        GROUP BY pickup_hour', porto_db)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pu.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pickup_weekday = pd.read_sql_query('SELECT Pickup_weekday, count(*) AS PUcount \\\n",
    "                        FROM taxidata_table_record \\\n",
    "                        GROUP BY pickup_weekday', porto_db)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pickup_weekday"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt; plt.rcdefaults()\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(7,7))\n",
    "objects = df_pickup_weekday['Pickup_weekday']\n",
    "y_pos = np.arange(len(objects))\n",
    "x_pos = df_pickup_weekday['PUcount']\n",
    " \n",
    "plt.bar(y_pos, x_pos, align='center', alpha=1.0)\n",
    "plt.xticks(y_pos, objects)\n",
    "plt.ylabel('Number of Pickups')\n",
    "plt.xlabel('weekdays')\n",
    "\n",
    "plt.title('Number of pickups during the week')\n",
    " \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Smoothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "porto_data.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "porto_data.tail(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gets the unique time bins where pickup values are present for each stand.\n",
    "\n",
    "def getUniqueBinsWithPickups(dataframe):\n",
    "    values = []\n",
    "    for i in range(63):          #we have total 63 clusters\n",
    "        stand_id = dataframe[dataframe[\"STAND\"] == (i+1)]\n",
    "        unique_stand_id = list(set(stand_id[\"TIME_BIN\"]))\n",
    "        unique_stand_id.sort()   #inplace sorting\n",
    "        values.append(unique_stand_id)\n",
    "    return values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill the missing value with 0's.\n",
    "def fillMissingWithZero(numberOfPickups, correspondingTimeBin):\n",
    "    ind = 0\n",
    "    smoothed_regions = []\n",
    "    for c in range(0, 63):\n",
    "        smoothed_bins = []\n",
    "        for t in range(17519):    #there are total 17519 time bins\n",
    "            if t in correspondingTimeBin[c]:   #if a time bin is present in \"correspondingTimeBin\" in stand 'c', \n",
    "            #then it means there is a pickup, in this case, we are simply adding number of pickups, else we are adding 0.\n",
    "                smoothed_bins.append(numberOfPickups[ind])\n",
    "                ind += 1\n",
    "            else:\n",
    "                smoothed_bins.append(0)\n",
    "        smoothed_regions.extend(smoothed_bins)\n",
    "    return smoothed_regions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def smoothing(numberOfPickups, correspondingTimeBin):\n",
    "    ind = 0\n",
    "    repeat = 0\n",
    "    smoothed_region = []\n",
    "    for cluster in range(0, 63):\n",
    "        smoothed_bin = []\n",
    "        for t1 in range(17519):\n",
    "            if repeat != 0:   #this will ensure that we shall not fill the pickup values again which we already filled by smoothing\n",
    "                repeat -= 1\n",
    "            else:\n",
    "                if t1 in correspondingTimeBin[cluster]:\n",
    "                    smoothed_bin.append(numberOfPickups[ind])\n",
    "                    ind += 1\n",
    "                else:\n",
    "                    if t1 == 0:           \n",
    "    #<---------------------CASE-1:Pickups missing in the beginning------------------------>\n",
    "                        for t2 in range(t1, 17519):\n",
    "                            if t2 not in correspondingTimeBin[cluster]:\n",
    "                                continue\n",
    "                            else:\n",
    "                                right_hand_limit = t2\n",
    "                                smoothed_value = (numberOfPickups[ind]*1.0)/((right_hand_limit + 1)*1.0)\n",
    "                                for i in range(right_hand_limit + 1):\n",
    "                                    smoothed_bin.append(math.ceil(smoothed_value))\n",
    "                                ind += 1\n",
    "                                repeat = right_hand_limit - t1\n",
    "                                \n",
    "                    if t1 != 0:\n",
    "                        right_hand_limit = 0\n",
    "                        for t2 in range(t1, 17519):\n",
    "                            if t2 not in correspondingTimeBin[cluster]:\n",
    "                                continue\n",
    "                            else:\n",
    "                                right_hand_limit = t2\n",
    "                                break\n",
    "                        if right_hand_limit == 0:\n",
    "    #<---------------------CASE-2: Pickups MISSING IN THE END------------------------------>\n",
    "                            smoothed_value = (numberOfPickups[ind-1]*1.0)/(((4464 - t1)+1)*1.0)\n",
    "                            del smoothed_bin[-1]\n",
    "                            for i in range((17519 - t1)+1):\n",
    "                                smoothed_bin.append(math.ceil(smoothed_value))\n",
    "                            repeat = (17519 - t1) - 1    \n",
    "    #<---------------------CASE-3: Pickups MISSING IN MIDDLE OF TWO VALUES----------------> \n",
    "                        else: \n",
    "                            smoothed_value = ((numberOfPickups[ind-1] + numberOfPickups[ind])*1.0)/(((right_hand_limit - t1)+2)*1.0)\n",
    "                            del smoothed_bin[-1]\n",
    "                            for i in range((right_hand_limit - t1)+2):\n",
    "                                smoothed_bin.append(math.ceil(smoothed_value))\n",
    "                            ind += 1\n",
    "                            repeat = right_hand_limit - t1                        \n",
    "        smoothed_region.extend(smoothed_bin)\n",
    "    return smoothed_region\n",
    "\n",
    "# when we multiply any integer with \"1.0\", then it will be converted into float."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def countZeros(num):\n",
    "    count = 0\n",
    "    for i in num:\n",
    "        if i == 0:\n",
    "            count += 1\n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_binswithPickup_porto_data = getUniqueBinsWithPickups(porto_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#unique_binswithPickup_porto_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "porto_data_fillZero = fillMissingWithZero(porto_data_timeBin_groupBy[\"CALL_TYPE\"].values, unique_binswithPickup_porto_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regionWisePickup_porto_data = []\n",
    "for i in range(63):\n",
    "    regionWisePickup_porto_data.append(porto_data_fillZero[17519*i:((17519*i)+17519)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(regionWisePickup_porto_data))\n",
    "print(len(regionWisePickup_porto_data[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(regionWisePickup_porto_data[62])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### POI Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_poidata=pd.read_csv(\"/home/tai/data/poi_populartimes.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "poi=pd.merge(df_poidata, taxistand, left_on='nearest_taxi_stand', right_on='Descricao', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "poi.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "poi.iloc[0]['Latitude']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "poi.loc[0,'ID']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=eval(poi.loc[0,'populartimes'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "str(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(a)):\n",
    "    print(a[i]['data'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(a[0]['data'][2:])\n",
    "for i in range(len(a)-2):\n",
    "    print(a[i+1]['data'])\n",
    "print(a[6]['data'][:]+a[0]['data'][0:2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_poidata.tail(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "poi.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frame_data.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frame_data.tail(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first pickup date/time: 2013-07-01 at 02-00-53 \n",
    "\n",
    "The last pickup date/time:2014-07-01 at 01-59-14\n",
    "\n",
    "Regarding to start/end pickup date/time we need to vectorize the populartimes in a time series with the same timestamp.\n",
    "\n",
    "We form standvisit matrix(63,17520) which indicates 63 timeseries for 63 taxistand. Each row shows the number of visitors around a taxi stand. In this case we aggregated populartimes of poi's. The radius is the distance between current taxistand to the nearest taxistand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "standvisit=np.zeros((63,17519 ))\n",
    "for p in range(len(poi)):\n",
    "    a=eval(poi.loc[p,'populartimes'])\n",
    "    visitlist=[]\n",
    "    \n",
    "    #visitlist should be started from Monday at 2:00 am and ended on Monday at 2:00 am\n",
    "    visitlist = a[0]['data'][2:]\n",
    "    for i in range(len(a)-2):\n",
    "        visitlist=visitlist+a[i+1]['data']\n",
    "    visitlist=visitlist+a[6]['data'][:]+a[0]['data'][0:2]\n",
    "    #we need visit data for the whole year \n",
    "    visitlist=visitlist*52\n",
    "    visitlist=visitlist+a[0]['data'][2:]+a[1]['data'][0:2]\n",
    "    #Since timestamp is 30 minute, we douplicate the visitlist\n",
    "    visitlist=np.repeat(visitlist, 2)\n",
    "    #The length of pickup timeseries is 17519, so we have to remove tha last 30-min timestamp from visitlist.\n",
    "    visitlist=visitlist[:17519]\n",
    "    #Add a new column to the poi dataset which stotes the visitlist for each place during the year\n",
    "    poi.loc[p,'visitors'] =str(visitlist)\n",
    "    #Agreggate the number of visits around each taxi stand. We calculate the total nymber of visits around a specific taxistand\n",
    "    s=poi.loc[p,'ID']\n",
    "    standvisit[s-1]=np.add(standvisit[s-1],visitlist)\n",
    "    #visitlist.clear()\n",
    "    #poi.apply(lambda s: eval(poi['visitors']), axis=1)\n",
    "    #arr.toarray().tolist()\n",
    "    #poi.groupby(['ID','Descricao'])['visitors'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(visitlist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b=poi.loc[0,'visitors']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "poi.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "taxistand.iloc[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del poi['visittimes']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "poi.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#taxistand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#taxistand.groupby(['Descricao'])['Descricao'].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(poi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "poi.tail(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# poi time series for each taxi stand\n",
    "standvisit.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "standvisit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assigning POI to the closest taxi stands"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "POI Dataset: This time we assign each pio to the nearest tasxistand, which mean we somehow define a zone nearby each taxistand. \n",
    "    In D2 dataset, we assign each trip to the closet taxi stand, we would like to use the same idea for pio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#There is no redundancy, which means we have 2051 distinct place in Porto\n",
    "poi['id'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=poi.groupby('ID')['ID'].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#a\n",
    "#np.sum(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#poi.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "taxistand.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from haversine import haversine\n",
    "from numpy import zeros\n",
    "dist = np.zeros((p,t))\n",
    "#dist is a 2051 * 63 matrix\n",
    "p=len(poi)\n",
    "t=len(taxistand)\n",
    "for plc in range(len(poi)):\n",
    "    slat=poi.loc[plc,\"latitude\"]\n",
    "    slon=poi.loc[plc,\"langitude\"]\n",
    "    loc1 = (slat, slon)\n",
    "    for txs in range(len(taxistand)):\n",
    "        elat=taxistand.loc[txs,\"Latitude\"]\n",
    "        elon=taxistand.loc[txs,\"Longitude\"]\n",
    "        loc2 = (elat, elon)\n",
    "        dist[plc][txs]=haversine(loc1, loc2)*1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist=np.array(np.round(dist), dtype=int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#poiZoneNumber is the closest taxi stand to a poi\n",
    "poiZoneNumber=np.zeros([2051,2])\n",
    "for d in range(len(dist)):\n",
    "    lst=dist[d,:]\n",
    "    poiZoneNumber[d][0]=int(np.min(lst))\n",
    "    poiZoneNumber[d][1]=np.argmin(lst)\n",
    "    #print(d,np.min(lst),np.argmin(lst))\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "poiZoneNumber=np.array(np.round(poiZoneNumber), dtype=int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "poiZoneNumber=pd.DataFrame(poiZoneNumber)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "poiZoneNumber.columns = ['dist2stand', 'poistand']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "poiZoneNumber.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "poiZoneNumber.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "poi.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "poidata=pd.concat([poi, poiZoneNumber], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ID\tDescricao\tLatitude\tLongitude are the features of nearest taxi stand when we assume that radious is the distance from target stand to the closest stand.\n",
    "# dist2stand\tpoistand are the features for nearest taxi stand when we assign each poi to the closest taxistand based on the distance.\n",
    "\n",
    "#Unnamed: 0\tid\tname\taddress\ttypes\tcoordinates\trating\trating_n\tinternational_phone_number\tpopulartimes\t...\tnearest_taxi_stand\tvisittimes\tlatitude\tlangitude are the features we extract from google map for each poi\n",
    "poidata.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(poidata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use this for prediction, by aggregrating all POIs assigned to taxi stand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newstandvisit=np.zeros((63,17519 ))\n",
    "for p in range(len(poidata)):\n",
    "    a=eval(poidata.loc[p,'populartimes'])\n",
    "    visitlist=[]\n",
    "    \n",
    "    #visitlist should be started from Monday at 2:00 am and ended on Monday at 2:00 am\n",
    "    visitlist = a[0]['data'][2:]\n",
    "    for i in range(len(a)-2):\n",
    "        visitlist=visitlist+a[i+1]['data']\n",
    "    visitlist=visitlist+a[6]['data'][:]+a[0]['data'][0:2]\n",
    "    #we need visit data for the whole year \n",
    "    visitlist=visitlist*52\n",
    "    visitlist=visitlist+a[0]['data'][2:]+a[1]['data'][0:2]\n",
    "    #Since timestamp is 30 minute, we douplicate the visitlist\n",
    "    visitlist=np.repeat(visitlist, 2)\n",
    "    #The length of pickup timeseries is 17519, so we have to remove tha last 30-min timestamp from visitlist.\n",
    "    visitlist=visitlist[:17519]\n",
    "    #Add a new column to the poi dataset which stotes the visitlist for each place during the year\n",
    "    poidata.loc[p,'newvisitors'] =str(visitlist)\n",
    "    #Agreggate the number of visits around each taxi stand. We calculate the total nymber of visits around a specific taxistand\n",
    "    s=poidata.loc[p,'poistand']\n",
    "    newstandvisit[s-1]=np.add(newstandvisit[s-1],visitlist)\n",
    "    #visitlist.clear()\n",
    "    #poi.apply(lambda s: eval(poi['visitors']), axis=1)\n",
    "    #arr.toarray().tolist()\n",
    "    #poi.groupby(['ID','Descricao'])['visitors'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newtotalvisit=np.sum(newstandvisit,axis=1,dtype=int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(newtotalvisit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newtotalvisit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "totalvisit=np.sum(standvisit,axis=1,dtype=int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(totalvisit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "totalvisit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Regression Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preparing data to be split into train and test, The below code prepares data in cumulative form which will be later split into\n",
    "test and train\n",
    "\n",
    "There are total 63 clusters and for the dataset and there are total 17519 time bins. \n",
    "For each stand there are 17519 time bins and so, for 63 clusters there will be 17519*63 pickup values because after \n",
    "smoothing each time bin has pickup.\n",
    "We will have a total of 17519x63 = 1.103.697 pickup values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we take number of pickups that are happened in last 10 30min intravels\n",
    "number_of_time_stamps = 10\n",
    "# TruePickups varaible\n",
    "# it is list of lists\n",
    "# It will be used as true labels/ground truth. Now since we are taking previous 10 pickups as a training data for predicting\n",
    "# next pickup(here next pickup will be a true/ground truth pickup), so \"TruePickups\" will not contain first five pickups of each \n",
    "# cluster. It will contain number of pickups 17519-10 = 17509 for each cluster. \n",
    "TruePickups = []\n",
    "\n",
    "# lat,lon will contain 17519-10=17509 times latitude of cluster center for every stand\n",
    "# it is list of lists\n",
    "lat = []\n",
    "lon = []\n",
    "# we will code each day \n",
    "# sunday = 0, monday=1, tue = 2, wed=3, thur=4, fri=5,sat=6\n",
    "day_of_week = []\n",
    "\n",
    "feat = []\n",
    "numberofstands = 63\n",
    "lengthoftimeseries= len(regionWisePickup_porto_data[0])\n",
    "\n",
    "feat = [0]*number_of_time_stamps\n",
    "for i in range(numberofstands):\n",
    "        lat.append([centerOfRegions[i][0]]*(lengthoftimeseries-number_of_time_stamps)) \n",
    "        lon.append([centerOfRegions[i][1]]*(lengthoftimeseries-number_of_time_stamps))\n",
    "        day_of_week.append([int(((int(j/48)%7)+number_of_time_stamps)%7) for j in range(number_of_time_stamps, lengthoftimeseries)])\n",
    "        #48 is the time bin for a day (30 min)\n",
    "        feat = np.vstack((feat, [regionWisePickup_porto_data[i][k:k+number_of_time_stamps] for k in range(0, len(regionWisePickup_porto_data[i]) - (number_of_time_stamps))]))\n",
    "        #feat = np.vstack(([regionWisePickup_porto_data[i][k:k+number_of_time_stamps] for k in range(0, len(regionWisePickup_porto_data[i]) - (number_of_time_stamps))]))\n",
    "        TruePickups.append(regionWisePickup_porto_data[i][number_of_time_stamps:])\n",
    "        #output contains pickup values of all the regions and of each time stamp, except first 5 time stamp pickups of each region.\n",
    "feat = feat[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(lon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(lon[0])*len(lon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(day_of_week[0])*len(day_of_week)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(lat[0])*len(lat) == len(lon[0])*len(lon) == len(day_of_week[0])*len(day_of_week) == 17509*63 == len(feat) == len(TruePickups[0])*len(TruePickups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(feat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding Predictions of Weighted Moving Average Predictions as a feature in our data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting the predictions of weighted moving averages to be used as a feature in cumulative form.\n",
    "\n",
    "Upto now we computed 8 features for every data point that starts from 50th min of the day.\n",
    "1. cluster center latitude\n",
    "2. cluster center longitude\n",
    "3. day of the week \n",
    "4. f_t_1: number of pickups that are happened previous t-1st 10min interval\n",
    "5. f_t_2: number of pickups that are happened previous t-2nd 10min interval\n",
    "6. f_t_3: number of pickups that are happened previous t-3rd 10min interval\n",
    "7. f_t_4: number of pickups that are happened previous t-4th 10min interval\n",
    "8. f_t_5: number of pickups that are happened previous t-5th 10min interval\n",
    "\n",
    "From the baseline models we said that the weighted moving avarage predictions gives us the best error.\n",
    "We will try to add the same weighted moving avarage predictions at time t as a feature to our data.<br>\n",
    "Weighted Moving Average -> $P_{t} = ( N*P_{t-1} + (N-1)*P_{t-2} + (N-2)*P_{t-3} .... 1*P_{t-n} )/(N*(N+1)/2)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"predicted_pickup_values\": it is a temporary array that store weighted moving avarag prediction values for each 30min intervl, \n",
    "# for each cluster it will get reset.\n",
    "# for every cluster it contains 17519 values\n",
    "predicted_pickup_values = []\n",
    "\n",
    "# \"predicted_pickup_values_list\"\n",
    "# it is list of lists\n",
    "predicted_pickup_values_list = []\n",
    "\n",
    "predicted_value = -1  #it will contain cuurent predicted_value. Default is given -1 which will be replaced later\n",
    "\n",
    "window_size = 2\n",
    "for i in range(numberofstands):\n",
    "    for j in range(lengthoftimeseries):\n",
    "        if j == 0:\n",
    "            predicted_value = regionWisePickup_porto_data[i][j]\n",
    "            predicted_pickup_values.append(0)\n",
    "        else:\n",
    "            if j>=window_size:\n",
    "                sumPickups = 0\n",
    "                sumOfWeights = 0\n",
    "                for k in range(window_size, 0, -1):\n",
    "                    sumPickups += k*(regionWisePickup_porto_data[i][j -window_size + (k - 1)])\n",
    "                    sumOfWeights += k\n",
    "                predicted_value = int(sumPickups/sumOfWeights)\n",
    "                predicted_pickup_values.append(predicted_value)\n",
    "            else:\n",
    "                sumPickups = 0\n",
    "                sumOfWeights = 0\n",
    "                for k in range(j, 0, -1):\n",
    "                    sumPickups += k*regionWisePickup_porto_data[i][k-1]\n",
    "                    sumOfWeights += k\n",
    "                predicted_value = int(sumPickups/sumOfWeights)\n",
    "                predicted_pickup_values.append(predicted_value)\n",
    "                \n",
    "    predicted_pickup_values_list.append(predicted_pickup_values[10:])\n",
    "    predicted_pickup_values = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(predicted_pickup_values_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(predicted_pickup_values_list[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#%matplotlib notebook\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "pp = PdfPages('stand_pattern.pdf')\n",
    "for i in range(63):#number of cluster\n",
    "    fig = plt.figure(figsize = (18, 6))\n",
    "    plt.plot(regionWisePickup_porto_data[i][:336])\n",
    "    plt.title(\"Pickup pattern of stand \"+str(i+1)+\" for 7 days (01/07/2013-07/07/2013)\")\n",
    "    plt.xlabel(\"30 Minute Time Bins\")\n",
    "    plt.ylabel(\"Number of Pickups\")\n",
    "    pp.savefig(fig)\n",
    "pp.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation for regression models\n",
    "Before we start predictions using the tree based regression models we take Jan 2016 pickup data and split it such that for every region we have 70% data in train and 30% in test, ordered date-wise for every region."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"size of total train data :\" +str(int(17509*63*0.7)))\n",
    "print(\"size of total test data :\" +str(int(17509*63*0.3)))\n",
    "trainsize =int((lengthoftimeseries - number_of_time_stamps)*numberofstands*0.7)\n",
    "testsize=int((lengthoftimeseries - number_of_time_stamps)*numberofstands*0.3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainsize\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testsize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"size of train data for one stand:\" +str(int(17509*0.7)))\n",
    "print(\"size of total test data for one stand:\" +str(int(17509*0.3)))\n",
    "trainsizeonestand=int((lengthoftimeseries - number_of_time_stamps)*0.7)\n",
    "testsizeonestand=int((lengthoftimeseries - number_of_time_stamps)*0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainsizeonestand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testsizeonestand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_previousFive_pickups  = [feat[(i*(lengthoftimeseries - number_of_time_stamps)):((lengthoftimeseries - number_of_time_stamps)*i+trainsizeonestand)] for i in range(numberofstands)]\n",
    "test_previousFive_pickups  = [feat[(i*(lengthoftimeseries - number_of_time_stamps)+trainsizeonestand):((lengthoftimeseries - number_of_time_stamps)*(i+1))] for i in range(numberofstands)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Train Data: Total number of clusters = {}. Number of points in each cluster = {}. Total number of training points = {}\".format(len(train_previousFive_pickups), len(train_previousFive_pickups[0]), len(train_previousFive_pickups)*len(train_previousFive_pickups[0])))\n",
    "print(\"Test Data: Total number of clusters = {}. Number of points in each cluster = {}. Total number of test points = {}\".format(len(test_previousFive_pickups), len(test_previousFive_pickups[0]), len(test_previousFive_pickups)*len(test_previousFive_pickups[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#taking 70% data as train data from each cluster\n",
    "train_lat = [i[:trainsizeonestand] for i in lat]\n",
    "train_lon = [i[:trainsizeonestand] for i in lon]\n",
    "train_weekDay = [i[:trainsizeonestand] for i in day_of_week]\n",
    "train_weighted_avg = [i[:trainsizeonestand] for i in predicted_pickup_values_list]\n",
    "train_TruePickups = [i[:trainsizeonestand] for i in TruePickups]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#taking 30% data as test data from each cluster\n",
    "test_lat = [i[trainsizeonestand:] for i in lat]\n",
    "test_lon = [i[trainsizeonestand:] for i in lon]\n",
    "test_weekDay = [i[trainsizeonestand:] for i in day_of_week]\n",
    "test_weighted_avg = [i[trainsizeonestand:] for i in predicted_pickup_values_list]\n",
    "test_TruePickups = [i[trainsizeonestand:] for i in TruePickups]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert from lists of lists of list to lists of list\n",
    "train_pickups = []\n",
    "test_pickups = []\n",
    "for i in range(numberofstands):\n",
    "    train_pickups.extend(train_previousFive_pickups[i])\n",
    "    test_pickups.extend(test_previousFive_pickups[i])    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#stacking pickups,frequencies and amplitudes horizontally.\n",
    "# a = [1,2,3,4,5]\n",
    "# b = [6,7,8,9,10]\n",
    "# c = [11,12,13,14,15]\n",
    "# d = np.hstack((a, b, c))\n",
    "# d = array([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15])\n",
    "\n",
    "#train_prevPickups_freq_amp = np.hstack((train_pickups, train_freq, train_amp))\n",
    "#test_prevPickups_freq_amp = np.hstack((test_pickups, test_freq, test_amp))\n",
    "#Khong su dung thong tin them\n",
    "train_prevPickups_freq_amp = train_pickups\n",
    "test_prevPickups_freq_amp = test_pickups\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of data points in train data = {}. Number of columns till now = {}\".format(len(train_prevPickups_freq_amp), len(train_prevPickups_freq_amp[0])))\n",
    "print(\"Number of data points in test data = {}. Number of columns till now = {}\".format(len(test_prevPickups_freq_amp), len(test_prevPickups_freq_amp[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting lists of lists into single list i.e flatten\n",
    "# a  = [[1,2,3,4],[4,6,7,8]]\n",
    "# print(sum(a,[]))\n",
    "# [1, 2, 3, 4, 4, 6, 7, 8]\n",
    "\n",
    "train_flat_lat = sum(train_lat, [])\n",
    "train_flat_lon = sum(train_lon, [])\n",
    "train_flat_weekDay = sum(train_weekDay, [])\n",
    "train_weighted_avg_flat = sum(train_weighted_avg, [])\n",
    "train_TruePickups_flat = sum(train_TruePickups, [])\n",
    "\n",
    "test_flat_lat = sum(test_lat, [])\n",
    "test_flat_lon = sum(test_lon, [])\n",
    "test_flat_weekDay = sum(test_weekDay, [])\n",
    "test_weighted_avg_flat = sum(test_weighted_avg, [])\n",
    "test_TruePickups_flat = sum(test_TruePickups, [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_prevPickups_freq_amp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(test_TruePickups_flat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_prevPickups_freq_amp[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualize data\n",
    "fig_pickup = plt.figure(figsize = (8, 6))\n",
    "plt.plot(train_TruePickups_flat[0:50])\n",
    "plt.title(\"Pickup Pattern for Jan-2016.\")\n",
    "plt.xlabel(\"30 Minute Time Bins\")\n",
    "plt.ylabel(\"Number of Pickups\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train dataframe\n",
    "#columns = ['ft_10','ft_9','ft_8','ft_7','ft_6','ft_5','ft_4','ft_3','ft_2','ft_1', 'freq1', 'freq2','freq3','freq4','freq5', 'Amp1', 'Amp2', 'Amp3', 'Amp4', 'Amp5']\n",
    "columns = ['ft_10','ft_9','ft_8','ft_7','ft_6','ft_5','ft_4','ft_3','ft_2','ft_1']\n",
    "Train_DF = pd.DataFrame(data = train_prevPickups_freq_amp, columns = columns)\n",
    "#Train_DF[\"Latitude\"] = train_flat_lat\n",
    "#Train_DF[\"Longitude\"] = train_flat_lon\n",
    "#Train_DF[\"WeekDay\"] = train_flat_weekDay\n",
    "#Train_DF[\"WeightedAvg\"] = train_weighted_avg_flat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test dataframe\n",
    "Test_DF = pd.DataFrame(data = test_prevPickups_freq_amp, columns = columns)\n",
    "#Test_DF[\"Latitude\"] = test_flat_lat\n",
    "#Test_DF[\"Longitude\"] = test_flat_lon\n",
    "#Test_DF[\"WeekDay\"] = test_flat_weekDay\n",
    "#Test_DF[\"WeightedAvg\"] = test_weighted_avg_flat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"Shape of train data = \"+str(Train_DF.shape))\n",
    "print(\"Shape of test data = \"+str(Test_DF.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Train_DF.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Test_DF.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define SMAPE\n",
    "#{\\displaystyle {\\text{SMAPE}}={\\frac {100\\%}{n}}\\sum _{t=1}^{n}{\\frac {|F_{t}-A_{t}|}{|A_{t}|+|F_{t}|+c}}}\n",
    "#in our case c=1, use laplace correction\n",
    "def smape(y_true, y_pred):\n",
    "    denominator = (np.abs(y_true) + np.abs(y_pred)+1) / 100.0\n",
    "    diff = np.abs(y_true - y_pred) / denominator\n",
    "    #diff[denominator == 0] = 1\n",
    "    return np.nanmean(diff)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lin_regression(train_data, train_true, test_data, test_true):\n",
    "    \n",
    "    #standardizing the data\n",
    "    train_std = StandardScaler().fit_transform(train_data)\n",
    "    test_std = StandardScaler().fit_transform(test_data)\n",
    "    \n",
    "    #hyper-paramater tuning\n",
    "    clf = SGDRegressor(loss = \"squared_loss\", penalty = \"l2\")\n",
    "    values = [10**-14, 10**-12, 10**-10, 10**-8, 10**-6, 10**-4, 10**-2, 10**0, 10**2, 10**4, 10**6]\n",
    "    hyper_parameter = {\"alpha\": values}\n",
    "    \n",
    "    best_parameter = GridSearchCV(clf, hyper_parameter, scoring = \"neg_mean_absolute_error\", cv = 3)\n",
    "    best_parameter.fit(train_std, train_true)\n",
    "    alpha = best_parameter.best_params_[\"alpha\"]\n",
    "    \n",
    "    #applying linear regression with best hyper-parameter\n",
    "    clf = SGDRegressor(loss = \"squared_loss\", penalty = \"l2\", alpha = alpha)\n",
    "    clf.fit(train_std, train_true)\n",
    "    train_pred = clf.predict(train_std)\n",
    "    train_MAPE = mean_absolute_error(train_true, train_pred)/ (sum(train_true)/len(train_true))\n",
    "    train_SMAPE = smape(train_true, train_pred)\n",
    "    train_MSE = mean_squared_error(train_true, train_pred)\n",
    "    train_RMSE = math.sqrt(train_MSE) ## bonus\n",
    "    test_pred = clf.predict(test_std)\n",
    "    test_MAPE = mean_absolute_error(test_true, test_pred)/ (sum(test_true)/len(test_true))\n",
    "    test_SMAPE = smape(test_true, test_pred)\n",
    "    test_MSE = mean_squared_error(test_true, test_pred)\n",
    "    test_RMSE = math.sqrt(test_MSE)\n",
    "    return train_MAPE, train_MSE, train_RMSE, train_SMAPE, test_MAPE, test_MSE, test_RMSE, test_SMAPE, alpha"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest Regressor function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def randomFor(train_data, train_true, test_data, test_true):\n",
    "    \n",
    "    #hyper-paramater tuning\n",
    "    values = [10, 40, 80, 150, 600, 800]\n",
    "    clf = RandomForestRegressor(n_jobs = -1)\n",
    "    hyper_parameter = {\"n_estimators\": values}\n",
    "    ##\n",
    "    hyper_parameter\n",
    "    ##\n",
    "    best_parameter = GridSearchCV(clf, hyper_parameter, scoring = \"neg_mean_absolute_error\", cv = 3)\n",
    "    best_parameter.fit(train_data, train_true)\n",
    "    estimators = best_parameter.best_params_[\"n_estimators\"]\n",
    "    \n",
    "    #applying random forest with best hyper-parameter\n",
    "    clf = RandomForestRegressor(n_estimators = estimators, n_jobs = -1)\n",
    "    clf.fit(train_data, train_true)\n",
    "    train_pred = clf.predict(train_data)\n",
    "    train_MAPE = mean_absolute_error(train_true, train_pred)/ (sum(train_true)/len(train_true))\n",
    "    train_SMAPE = smape(train_true, train_pred)\n",
    "    train_MSE = mean_squared_error(train_true, train_pred)\n",
    "    train_RMSE = math.sqrt(train_MSE)\n",
    "    test_pred = clf.predict(test_data)\n",
    "    test_MAPE = mean_absolute_error(test_true, test_pred)/ (sum(test_true)/len(test_true))\n",
    "    test_SMAPE = smape(test_true, test_pred)\n",
    "    test_MSE = mean_squared_error(test_true, test_pred)\n",
    "    test_RMSE = math.sqrt(test_MSE)\n",
    "    return train_MAPE, train_MSE, train_RMSE, train_SMAPE, test_MAPE, test_MSE, test_RMSE, test_SMAPE,estimators"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost Regressor function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def xgboost_reg(train_data, train_true, test_data, test_true):\n",
    "    #hyper-parameter tuning\n",
    "    hyper_parameter = {\"max_depth\":[1, 2, 3, 4], \"n_estimators\":[40, 80, 150, 600]}\n",
    "    clf = xgb.XGBRegressor()\n",
    "    best_parameter = GridSearchCV(clf, hyper_parameter, scoring = \"neg_mean_absolute_error\", cv = 3)\n",
    "    best_parameter.fit(train_data, train_true)\n",
    "    estimators = best_parameter.best_params_[\"n_estimators\"]\n",
    "    depth = best_parameter.best_params_[\"max_depth\"]\n",
    "    \n",
    "    #applying xgboost regressor with best hyper-parameter\n",
    "    clf = xgb.XGBRegressor(max_depth = depth, n_estimators = estimators)\n",
    "    clf.fit(train_data, train_true)\n",
    "    train_pred = clf.predict(train_data)\n",
    "    train_MAPE = mean_absolute_error(train_true, train_pred)/ (sum(train_true)/len(train_true))\n",
    "    train_SMAPE = smape(train_true, train_pred)\n",
    "    train_MSE = mean_squared_error(train_true, train_pred)\n",
    "    train_RMSE = math.sqrt(train_MSE)\n",
    "    test_pred = clf.predict(test_data)\n",
    "    test_MAPE = mean_absolute_error(test_true, test_pred)/ (sum(test_true)/len(test_true))\n",
    "    test_SMAPE = smape(test_true, test_pred)\n",
    "    test_MSE = mean_squared_error(test_true, test_pred)\n",
    "    test_RMSE = math.sqrt(test_MSE)\n",
    "    return train_MAPE, train_MSE, train_RMSE, train_SMAPE, test_MAPE, test_MSE, test_RMSE,test_SMAPE,depth,estimators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainMAPE_lr, trainMSE_lr, train_RMSE_lr, train_SMAPE_lr,testMAPE_lr, testMSE_lr, test_RMSE_lr,test_SMAPE_lr = lin_regression(Train_DF, train_TruePickups_flat, Test_DF, test_TruePickups_flat)\n",
    "trainMAPE_rf, trainMSE_rf, train_RMSE_rf, train_SMAPE_rf,testMAPE_rf, testMSE_rf, test_RMSE_rf,test_SMAPE_rf = randomFor(Train_DF, train_TruePickups_flat, Test_DF, test_TruePickups_flat)\n",
    "trainMAPE_xgb, trainMSE_xgb, train_RMSE_xgb, train_SMAPE_xgb,testMAPE_xgb, testMSE_xgb, test_RMSE_xgb,test_SMAPE_xgb = xgboost_reg(Train_DF, train_TruePickups_flat, Test_DF, test_TruePickups_flat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "error_table_regressions = pd.DataFrame(columns = [\"Model\", \"TrainMAPE(%)\", \"TrainMSE\", \"TrainRMSE\",\"TrainSMAPE(%)\",\"TestMAPE(%)\", \"TestMSE\", \"TestRMSE\",\"TestSMAPE(%)\"])\n",
    "\n",
    "error_table_regressions = error_table_regressions.append(pd.DataFrame([[\"Linear Regression\", trainMAPE_lr*100, trainMSE_lr, train_RMSE_lr, train_SMAPE_lr, testMAPE_lr*100, testMSE_lr,test_RMSE_lr,test_SMAPE_lr ]], columns = [\"Model\", \"TrainMAPE(%)\", \"TrainMSE\", \"TrainRMSE\",\"TrainSMAPE(%)\",\"TestMAPE(%)\", \"TestMSE\",\"TestRMSE\",\"TestSMAPE(%)\"]))\n",
    "error_table_regressions = error_table_regressions.append(pd.DataFrame([[\"Random Forest Regression\", trainMAPE_rf*100, trainMSE_rf, train_RMSE_rf, train_SMAPE_rf, testMAPE_rf*100, testMSE_rf, test_RMSE_rf, test_SMAPE_rf]], columns = [\"Model\", \"TrainMAPE(%)\", \"TrainMSE\",\"TrainRMSE\",\"TrainSMAPE(%)\", \"TestMAPE(%)\", \"TestMSE\",\"TestRMSE\",\"TestSMAPE(%)\"]))\n",
    "error_table_regressions = error_table_regressions.append(pd.DataFrame([[\"XGBoost Regressor\", trainMAPE_xgb*100, trainMSE_xgb,train_RMSE_xgb, train_SMAPE_xgb, testMAPE_xgb*100, testMSE_xgb,test_RMSE_xgb, test_SMAPE_xgb]], columns = [\"Model\", \"TrainMAPE(%)\", \"TrainMSE\", \"TrainRMSE\",\"TrainSMAPE(%)\",\"TestMAPE(%)\", \"TestMSE\",\"TestRMSE\",\"TestSMAPE(%)\"]))\n",
    "error_table_regressions.reset_index(drop = True, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "error_table_regressions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finding parameter for model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_number_of_time_stamps_lr=2\n",
    "min_sMAPE_lr=100\n",
    "\n",
    "best_number_of_time_stamps_rf=2\n",
    "min_sMAPE_rf=100\n",
    "\n",
    "best_number_of_time_stamps_xgb=2\n",
    "min_sMAPE_xgb=100\n",
    "\n",
    "\n",
    "for number_of_time_stamps in range(2, 20):    \n",
    "    # TruePickups varaible\n",
    "    # it is list of lists\n",
    "    # It will be used as true labels/ground truth. Now since we are taking previous 10 pickups as a training data for predicting\n",
    "    # next pickup(here next pickup will be a true/ground truth pickup), so \"TruePickups\" will not contain first five pickups of each \n",
    "    # cluster. It will contain number of pickups 17519-10 = 17509 for each cluster. \n",
    "    TruePickups = []\n",
    "\n",
    "    # lat,lon will contain 17519-10=17509 times latitude of cluster center for every stand\n",
    "    # it is list of lists\n",
    "    lat = []\n",
    "    lon = []\n",
    "    # we will code each day \n",
    "    # sunday = 0, monday=1, tue = 2, wed=3, thur=4, fri=5,sat=6\n",
    "    day_of_week = []\n",
    "\n",
    "    feat = []\n",
    "    numberofstands = 1\n",
    "    lengthoftimeseries= int(len(regionWisePickup_porto_data[0])/1)\n",
    "\n",
    "    feat = [0]*number_of_time_stamps\n",
    "    for i in range(numberofstands):\n",
    "            lat.append([centerOfRegions[i][0]]*(lengthoftimeseries-number_of_time_stamps)) \n",
    "            lon.append([centerOfRegions[i][1]]*(lengthoftimeseries-number_of_time_stamps))\n",
    "            day_of_week.append([int(((int(j/48)%7)+number_of_time_stamps)%7) for j in range(number_of_time_stamps, lengthoftimeseries)])\n",
    "            #48 is the time bin for a day (30 min)\n",
    "            feat = np.vstack((feat, [regionWisePickup_porto_data[i][k:k+number_of_time_stamps] for k in range(0, len(regionWisePickup_porto_data[i]) - (number_of_time_stamps))]))\n",
    "            #feat = np.vstack(([regionWisePickup_porto_data[i][k:k+number_of_time_stamps] for k in range(0, len(regionWisePickup_porto_data[i]) - (number_of_time_stamps))]))\n",
    "            TruePickups.append(regionWisePickup_porto_data[i][number_of_time_stamps:])\n",
    "            #output contains pickup values of all the regions and of each time stamp, except first 5 time stamp pickups of each region.\n",
    "    feat = feat[1:]\n",
    "    # \"predicted_pickup_values\": it is a temporary array that store weighted moving avarag prediction values for each 30min intervl, \n",
    "    # for each cluster it will get reset.\n",
    "    # for every cluster it contains 17519 values\n",
    "    predicted_pickup_values = []\n",
    "\n",
    "    # \"predicted_pickup_values_list\"\n",
    "    # it is list of lists\n",
    "    predicted_pickup_values_list = []\n",
    "\n",
    "    predicted_value = -1  #it will contain cuurent predicted_value. Default is given -1 which will be replaced later\n",
    "\n",
    "    window_size = 2\n",
    "    for i in range(numberofstands):\n",
    "        for j in range(lengthoftimeseries):\n",
    "            if j == 0:\n",
    "                predicted_value = regionWisePickup_porto_data[i][j]\n",
    "                predicted_pickup_values.append(0)\n",
    "            else:\n",
    "                if j>=window_size:\n",
    "                    sumPickups = 0\n",
    "                    sumOfWeights = 0\n",
    "                    for k in range(window_size, 0, -1):\n",
    "                        sumPickups += k*(regionWisePickup_porto_data[i][j -window_size + (k - 1)])\n",
    "                        sumOfWeights += k\n",
    "                    predicted_value = int(sumPickups/sumOfWeights)\n",
    "                    predicted_pickup_values.append(predicted_value)\n",
    "                else:\n",
    "                    sumPickups = 0\n",
    "                    sumOfWeights = 0\n",
    "                    for k in range(j, 0, -1):\n",
    "                        sumPickups += k*regionWisePickup_porto_data[i][k-1]\n",
    "                        sumOfWeights += k\n",
    "                    predicted_value = int(sumPickups/sumOfWeights)\n",
    "                    predicted_pickup_values.append(predicted_value)\n",
    "\n",
    "        predicted_pickup_values_list.append(predicted_pickup_values[10:])\n",
    "        predicted_pickup_values = []\n",
    "    trainsize =int((lengthoftimeseries - number_of_time_stamps)*numberofstands*0.7)\n",
    "    testsize=int((lengthoftimeseries - number_of_time_stamps)*numberofstands*0.3)\n",
    "    trainsizeonestand=int((lengthoftimeseries - number_of_time_stamps)*0.7)\n",
    "    testsizeonestand=int((lengthoftimeseries - number_of_time_stamps)*0.3)\n",
    "    train_previousFive_pickups  = [feat[(i*(lengthoftimeseries - number_of_time_stamps)):((lengthoftimeseries - number_of_time_stamps)*i+trainsizeonestand)] for i in range(numberofstands)]\n",
    "    test_previousFive_pickups  = [feat[(i*(lengthoftimeseries - number_of_time_stamps)+trainsizeonestand):((lengthoftimeseries - number_of_time_stamps)*(i+1))] for i in range(numberofstands)]\n",
    "    print(\"Train Data: Total number of clusters = {}. Number of points in each cluster = {}. Total number of training points = {}\".format(len(train_previousFive_pickups), len(train_previousFive_pickups[0]), len(train_previousFive_pickups)*len(train_previousFive_pickups[0])))\n",
    "    print(\"Test Data: Total number of clusters = {}. Number of points in each cluster = {}. Total number of test points = {}\".format(len(test_previousFive_pickups), len(test_previousFive_pickups[0]), len(test_previousFive_pickups)*len(test_previousFive_pickups[0])))\n",
    "    #taking 70% data as train data from each cluster\n",
    "    train_lat = [i[:trainsizeonestand] for i in lat]\n",
    "    train_lon = [i[:trainsizeonestand] for i in lon]\n",
    "    train_weekDay = [i[:trainsizeonestand] for i in day_of_week]\n",
    "    train_weighted_avg = [i[:trainsizeonestand] for i in predicted_pickup_values_list]\n",
    "    train_TruePickups = [i[:trainsizeonestand] for i in TruePickups]\n",
    "    #taking 30% data as test data from each cluster\n",
    "    test_lat = [i[trainsizeonestand:] for i in lat]\n",
    "    test_lon = [i[trainsizeonestand:] for i in lon]\n",
    "    test_weekDay = [i[trainsizeonestand:] for i in day_of_week]\n",
    "    test_weighted_avg = [i[trainsizeonestand:] for i in predicted_pickup_values_list]\n",
    "    test_TruePickups = [i[trainsizeonestand:] for i in TruePickups]\n",
    "    # convert from lists of lists of list to lists of list\n",
    "    train_pickups = []\n",
    "    test_pickups = []\n",
    "    for i in range(numberofstands):\n",
    "        train_pickups.extend(train_previousFive_pickups[i])\n",
    "        test_pickups.extend(test_previousFive_pickups[i])    \n",
    "    train_prevPickups_freq_amp = train_pickups\n",
    "    test_prevPickups_freq_amp = test_pickups\n",
    "    print(\"Number of data points in train data = {}. Number of columns till now = {}\".format(len(train_prevPickups_freq_amp), len(train_prevPickups_freq_amp[0])))\n",
    "    print(\"Number of data points in test data = {}. Number of columns till now = {}\".format(len(test_prevPickups_freq_amp), len(test_prevPickups_freq_amp[0])))\n",
    "    # converting lists of lists into single list i.e flatten\n",
    "    # a  = [[1,2,3,4],[4,6,7,8]]\n",
    "    # print(sum(a,[]))\n",
    "    # [1, 2, 3, 4, 4, 6, 7, 8]\n",
    "\n",
    "    train_flat_lat = sum(train_lat, [])\n",
    "    train_flat_lon = sum(train_lon, [])\n",
    "    train_flat_weekDay = sum(train_weekDay, [])\n",
    "    train_weighted_avg_flat = sum(train_weighted_avg, [])\n",
    "    train_TruePickups_flat = sum(train_TruePickups, [])\n",
    "\n",
    "    test_flat_lat = sum(test_lat, [])\n",
    "    test_flat_lon = sum(test_lon, [])\n",
    "    test_flat_weekDay = sum(test_weekDay, [])\n",
    "    test_weighted_avg_flat = sum(test_weighted_avg, [])\n",
    "    test_TruePickups_flat = sum(test_TruePickups, [])\n",
    "    columns=[]\n",
    "    for i in range(number_of_time_stamps):\n",
    "        ftname=\"ft\"+str(i)\n",
    "        columns.append(ftname)\n",
    "    #columns = ['ft_10','ft_9','ft_8','ft_7','ft_6','ft_5','ft_4','ft_3','ft_2','ft_1']\n",
    "    Train_DF = pd.DataFrame(data = train_prevPickups_freq_amp, columns = columns)\n",
    "    Test_DF = pd.DataFrame(data = test_prevPickups_freq_amp, columns = columns)\n",
    "\n",
    "    trainMAPE_lr, trainMSE_lr, train_RMSE_lr, train_SMAPE_lr,testMAPE_lr, testMSE_lr, test_RMSE_lr,test_SMAPE_lr = lin_regression(Train_DF, train_TruePickups_flat, Test_DF, test_TruePickups_flat)\n",
    "    trainMAPE_rf, trainMSE_rf, train_RMSE_rf, train_SMAPE_rf,testMAPE_rf, testMSE_rf, test_RMSE_rf,test_SMAPE_rf = randomFor(Train_DF, train_TruePickups_flat, Test_DF, test_TruePickups_flat)\n",
    "    trainMAPE_xgb, trainMSE_xgb, train_RMSE_xgb, train_SMAPE_xgb,testMAPE_xgb, testMSE_xgb, test_RMSE_xgb,test_SMAPE_xgb = xgboost_reg(Train_DF, train_TruePickups_flat, Test_DF, test_TruePickups_flat)\n",
    "    if (test_SMAPE_lr<min_sMAPE_lr):\n",
    "        min_sMAPE_lr=test_SMAPE_lr\n",
    "        best_number_of_time_stamps_lr = number_of_time_stamps\n",
    "    if (test_SMAPE_rf<min_sMAPE_rf):\n",
    "        min_sMAPE_rf=test_SMAPE_rf\n",
    "        best_number_of_time_stamps_rf = number_of_time_stamps\n",
    "    if (test_SMAPE_xgb<min_sMAPE_xgb):\n",
    "        min_sMAPE_xgb=test_SMAPE_xgb\n",
    "        best_number_of_time_stamps_xgb = number_of_time_stamps\n",
    "print(\"The best number_of_time_stamps:  \" + str(best_number_of_time_stamps_lr) + \"with best sMAPE: \" + str(min_sMAPE_lr))\n",
    "print(\"The best number_of_time_stamps: \" + str(best_number_of_time_stamps_rf) + \"with best sMAPE: \" + str(min_sMAPE_rf))\n",
    "print(\"The best number_of_time_stamps: \" + str(best_number_of_time_stamps_xgb) + \"with best sMAPE: \" + str(min_sMAPE_xgb))   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run baseline models "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "    # TruePickups varaible\n",
    "    # it is list of lists\n",
    "    # It will be used as true labels/ground truth. Now since we are taking previous 10 pickups as a training data for predicting\n",
    "    # next pickup(here next pickup will be a true/ground truth pickup), so \"TruePickups\" will not contain first five pickups of each \n",
    "    # cluster. It will contain number of pickups 17519-10 = 17509 for each cluster. \n",
    "    TruePickups = []\n",
    "    #number_of_time_stamps = 15 #Linear\n",
    "    #number_of_time_stamps = 12 #Random forest\n",
    "    number_of_time_stamps = 19 #XGBoost\n",
    "    # lat,lon will contain 17519-10=17509 times latitude of cluster center for every stand\n",
    "    # it is list of lists\n",
    "    lat = []\n",
    "    lon = []\n",
    "    # we will code each day \n",
    "    # sunday = 0, monday=1, tue = 2, wed=3, thur=4, fri=5,sat=6\n",
    "    day_of_week = []\n",
    "\n",
    "    feat = []\n",
    "    numberofstands = 63 #Number of taxi stand\n",
    "    lengthoftimeseries= int(len(regionWisePickup_porto_data[0])/1)\n",
    "\n",
    "    feat = [0]*number_of_time_stamps\n",
    "    for i in range(numberofstands):\n",
    "            lat.append([centerOfRegions[i][0]]*(lengthoftimeseries-number_of_time_stamps)) \n",
    "            lon.append([centerOfRegions[i][1]]*(lengthoftimeseries-number_of_time_stamps))\n",
    "            day_of_week.append([int(((int(j/48)%7)+number_of_time_stamps)%7) for j in range(number_of_time_stamps, lengthoftimeseries)])\n",
    "            #48 is the time bin for a day (30 min)\n",
    "            feat = np.vstack((feat, [regionWisePickup_porto_data[i][k:k+number_of_time_stamps] for k in range(0, len(regionWisePickup_porto_data[i]) - (number_of_time_stamps))]))\n",
    "            #feat = np.vstack(([regionWisePickup_porto_data[i][k:k+number_of_time_stamps] for k in range(0, len(regionWisePickup_porto_data[i]) - (number_of_time_stamps))]))\n",
    "            TruePickups.append(regionWisePickup_porto_data[i][number_of_time_stamps:])\n",
    "            #output contains pickup values of all the regions and of each time stamp, except first 5 time stamp pickups of each region.\n",
    "    feat = feat[1:]\n",
    "    # \"predicted_pickup_values\": it is a temporary array that store weighted moving avarag prediction values for each 30min intervl, \n",
    "    # for each cluster it will get reset.\n",
    "    # for every cluster it contains 17519 values\n",
    "    predicted_pickup_values = []\n",
    "\n",
    "    # \"predicted_pickup_values_list\"\n",
    "    # it is list of lists\n",
    "    predicted_pickup_values_list = []\n",
    "\n",
    "    predicted_value = -1  #it will contain cuurent predicted_value. Default is given -1 which will be replaced later\n",
    "\n",
    "    window_size = 2\n",
    "    for i in range(numberofstands):\n",
    "        for j in range(lengthoftimeseries):\n",
    "            if j == 0:\n",
    "                predicted_value = regionWisePickup_porto_data[i][j]\n",
    "                predicted_pickup_values.append(0)\n",
    "            else:\n",
    "                if j>=window_size:\n",
    "                    sumPickups = 0\n",
    "                    sumOfWeights = 0\n",
    "                    for k in range(window_size, 0, -1):\n",
    "                        sumPickups += k*(regionWisePickup_porto_data[i][j -window_size + (k - 1)])\n",
    "                        sumOfWeights += k\n",
    "                    predicted_value = int(sumPickups/sumOfWeights)\n",
    "                    predicted_pickup_values.append(predicted_value)\n",
    "                else:\n",
    "                    sumPickups = 0\n",
    "                    sumOfWeights = 0\n",
    "                    for k in range(j, 0, -1):\n",
    "                        sumPickups += k*regionWisePickup_porto_data[i][k-1]\n",
    "                        sumOfWeights += k\n",
    "                    predicted_value = int(sumPickups/sumOfWeights)\n",
    "                    predicted_pickup_values.append(predicted_value)\n",
    "\n",
    "        predicted_pickup_values_list.append(predicted_pickup_values[10:])\n",
    "        predicted_pickup_values = []\n",
    "    trainsize =int((lengthoftimeseries - number_of_time_stamps)*numberofstands*0.7)\n",
    "    testsize=int((lengthoftimeseries - number_of_time_stamps)*numberofstands*0.3)\n",
    "    trainsizeonestand=int((lengthoftimeseries - number_of_time_stamps)*0.7)\n",
    "    testsizeonestand=int((lengthoftimeseries - number_of_time_stamps)*0.3)\n",
    "    train_previousFive_pickups  = [feat[(i*(lengthoftimeseries - number_of_time_stamps)):((lengthoftimeseries - number_of_time_stamps)*i+trainsizeonestand)] for i in range(numberofstands)]\n",
    "    test_previousFive_pickups  = [feat[(i*(lengthoftimeseries - number_of_time_stamps)+trainsizeonestand):((lengthoftimeseries - number_of_time_stamps)*(i+1))] for i in range(numberofstands)]\n",
    "    print(\"Train Data: Total number of clusters = {}. Number of points in each cluster = {}. Total number of training points = {}\".format(len(train_previousFive_pickups), len(train_previousFive_pickups[0]), len(train_previousFive_pickups)*len(train_previousFive_pickups[0])))\n",
    "    print(\"Test Data: Total number of clusters = {}. Number of points in each cluster = {}. Total number of test points = {}\".format(len(test_previousFive_pickups), len(test_previousFive_pickups[0]), len(test_previousFive_pickups)*len(test_previousFive_pickups[0])))\n",
    "    #taking 70% data as train data from each cluster\n",
    "    train_lat = [i[:trainsizeonestand] for i in lat]\n",
    "    train_lon = [i[:trainsizeonestand] for i in lon]\n",
    "    train_weekDay = [i[:trainsizeonestand] for i in day_of_week]\n",
    "    train_weighted_avg = [i[:trainsizeonestand] for i in predicted_pickup_values_list]\n",
    "    train_TruePickups = [i[:trainsizeonestand] for i in TruePickups]\n",
    "    #taking 30% data as test data from each cluster\n",
    "    test_lat = [i[trainsizeonestand:] for i in lat]\n",
    "    test_lon = [i[trainsizeonestand:] for i in lon]\n",
    "    test_weekDay = [i[trainsizeonestand:] for i in day_of_week]\n",
    "    test_weighted_avg = [i[trainsizeonestand:] for i in predicted_pickup_values_list]\n",
    "    test_TruePickups = [i[trainsizeonestand:] for i in TruePickups]\n",
    "    # convert from lists of lists of list to lists of list\n",
    "    train_pickups = []\n",
    "    test_pickups = []\n",
    "    for i in range(numberofstands):\n",
    "        train_pickups.extend(train_previousFive_pickups[i])\n",
    "        test_pickups.extend(test_previousFive_pickups[i])    \n",
    "    train_prevPickups_freq_amp = train_pickups\n",
    "    test_prevPickups_freq_amp = test_pickups\n",
    "    print(\"Number of data points in train data = {}. Number of columns till now = {}\".format(len(train_prevPickups_freq_amp), len(train_prevPickups_freq_amp[0])))\n",
    "    print(\"Number of data points in test data = {}. Number of columns till now = {}\".format(len(test_prevPickups_freq_amp), len(test_prevPickups_freq_amp[0])))\n",
    "    # converting lists of lists into single list i.e flatten\n",
    "    # a  = [[1,2,3,4],[4,6,7,8]]\n",
    "    # print(sum(a,[]))\n",
    "    # [1, 2, 3, 4, 4, 6, 7, 8]\n",
    "\n",
    "    train_flat_lat = sum(train_lat, [])\n",
    "    train_flat_lon = sum(train_lon, [])\n",
    "    train_flat_weekDay = sum(train_weekDay, [])\n",
    "    train_weighted_avg_flat = sum(train_weighted_avg, [])\n",
    "    train_TruePickups_flat = sum(train_TruePickups, [])\n",
    "\n",
    "    test_flat_lat = sum(test_lat, [])\n",
    "    test_flat_lon = sum(test_lon, [])\n",
    "    test_flat_weekDay = sum(test_weekDay, [])\n",
    "    test_weighted_avg_flat = sum(test_weighted_avg, [])\n",
    "    test_TruePickups_flat = sum(test_TruePickups, [])\n",
    "    columns=[]\n",
    "    for i in range(number_of_time_stamps):\n",
    "        ftname=\"ft\"+str(i)\n",
    "        columns.append(ftname)\n",
    "    #columns = ['ft_10','ft_9','ft_8','ft_7','ft_6','ft_5','ft_4','ft_3','ft_2','ft_1']\n",
    "    Train_DF = pd.DataFrame(data = train_prevPickups_freq_amp, columns = columns)\n",
    "    Test_DF = pd.DataFrame(data = test_prevPickups_freq_amp, columns = columns)\n",
    "\n",
    "    #trainMAPE_lr, trainMSE_lr, train_RMSE_lr, train_SMAPE_lr,testMAPE_lr, testMSE_lr, test_RMSE_lr,test_SMAPE_lr = lin_regression(Train_DF, train_TruePickups_flat, Test_DF, test_TruePickups_flat)\n",
    "    #rainMAPE_rf, trainMSE_rf, train_RMSE_rf, train_SMAPE_rf,testMAPE_rf, testMSE_rf, test_RMSE_rf,test_SMAPE_rf = randomFor(Train_DF, train_TruePickups_flat, Test_DF, test_TruePickups_flat)\n",
    "    trainMAPE_xgb, trainMSE_xgb, train_RMSE_xgb, train_SMAPE_xgb,testMAPE_xgb, testMSE_xgb, test_RMSE_xgb,test_SMAPE_xgb = xgboost_reg(Train_DF, train_TruePickups_flat, Test_DF, test_TruePickups_flat)    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "error_table_regressions = pd.DataFrame(columns = [\"Model\", \"TrainMAPE(%)\", \"TrainMSE\", \"TrainRMSE\",\"TrainSMAPE(%)\",\"TestMAPE(%)\", \"TestMSE\", \"TestRMSE\",\"TestSMAPE(%)\"])\n",
    "error_table_regressions = error_table_regressions.append(pd.DataFrame([[\"Linear Regression\", trainMAPE_lr*100, trainMSE_lr, train_RMSE_lr, train_SMAPE_lr, testMAPE_lr*100, testMSE_lr,test_RMSE_lr,test_SMAPE_lr ]], columns = [\"Model\", \"TrainMAPE(%)\", \"TrainMSE\", \"TrainRMSE\",\"TrainSMAPE(%)\",\"TestMAPE(%)\", \"TestMSE\",\"TestRMSE\",\"TestSMAPE(%)\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "error_table_regressions = error_table_regressions.append(pd.DataFrame([[\"Random Forest Regression\", trainMAPE_rf*100, trainMSE_rf, train_RMSE_rf, train_SMAPE_rf, testMAPE_rf*100, testMSE_rf, test_RMSE_rf, test_SMAPE_rf]], columns = [\"Model\", \"TrainMAPE(%)\", \"TrainMSE\",\"TrainRMSE\",\"TrainSMAPE(%)\", \"TestMAPE(%)\", \"TestMSE\",\"TestRMSE\",\"TestSMAPE(%)\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "error_table_regressions = error_table_regressions.append(pd.DataFrame([[\"XGBoost Regressor\", trainMAPE_xgb*100, trainMSE_xgb,train_RMSE_xgb, train_SMAPE_xgb, testMAPE_xgb*100, testMSE_xgb,test_RMSE_xgb, test_SMAPE_xgb]], columns = [\"Model\", \"TrainMAPE(%)\", \"TrainMSE\", \"TrainRMSE\",\"TrainSMAPE(%)\",\"TestMAPE(%)\", \"TestMSE\",\"TestRMSE\",\"TestSMAPE(%)\"]))\n",
    "error_table_regressions.reset_index(drop = True, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "error_table_regressions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numberoftaxistands = 63\n",
    "atrainMSE_lr = []\n",
    "atestMSE_lr = []\n",
    "atrain_SMAPE_lr =[]\n",
    "atest_SMAPE_lr = []\n",
    "aalpha_lr=[]\n",
    "for ii in range(numberoftaxistands):\n",
    "    print(\"Stand: \" + str(ii+1))\n",
    "    # TruePickups varaible\n",
    "    # it is list of lists\n",
    "    # It will be used as true labels/ground truth. Now since we are taking previous 10 pickups as a training data for predicting\n",
    "    # next pickup(here next pickup will be a true/ground truth pickup), so \"TruePickups\" will not contain first five pickups of each \n",
    "    # cluster. It will contain number of pickups 17519-10 = 17509 for each cluster. \n",
    "    TruePickups = []\n",
    "    number_of_time_stamps = 15 #Linear\n",
    "    #number_of_time_stamps = 12 #Random forest\n",
    "    #number_of_time_stamps = 19 #XGBoost\n",
    "    # lat,lon will contain 17519-10=17509 times latitude of cluster center for every stand\n",
    "    # it is list of lists\n",
    "    lat = []\n",
    "    lon = []\n",
    "    # we will code each day \n",
    "    # sunday = 0, monday=1, tue = 2, wed=3, thur=4, fri=5,sat=6\n",
    "    day_of_week = []\n",
    "\n",
    "    feat = []\n",
    "    numberofstands = 1 #Number of taxi stand\n",
    "    lengthoftimeseries= int(len(regionWisePickup_porto_data[0])/1)\n",
    "\n",
    "    feat = [0]*number_of_time_stamps\n",
    "    for i in range(numberofstands):\n",
    "            lat.append([centerOfRegions[ii][0]]*(lengthoftimeseries-number_of_time_stamps)) \n",
    "            lon.append([centerOfRegions[ii][1]]*(lengthoftimeseries-number_of_time_stamps))\n",
    "            day_of_week.append([int(((int(j/48)%7)+number_of_time_stamps)%7) for j in range(number_of_time_stamps, lengthoftimeseries)])\n",
    "            #48 is the time bin for a day (30 min)\n",
    "            feat = np.vstack((feat, [regionWisePickup_porto_data[ii][k:k+number_of_time_stamps] for k in range(0, len(regionWisePickup_porto_data[ii]) - (number_of_time_stamps))]))\n",
    "            #feat = np.vstack(([regionWisePickup_porto_data[i][k:k+number_of_time_stamps] for k in range(0, len(regionWisePickup_porto_data[i]) - (number_of_time_stamps))]))\n",
    "            TruePickups.append(regionWisePickup_porto_data[ii][number_of_time_stamps:])\n",
    "            #output contains pickup values of all the regions and of each time stamp, except first 5 time stamp pickups of each region.\n",
    "    feat = feat[1:]\n",
    "    # \"predicted_pickup_values\": it is a temporary array that store weighted moving avarag prediction values for each 30min intervl, \n",
    "    # for each cluster it will get reset.\n",
    "    # for every cluster it contains 17519 values\n",
    "    predicted_pickup_values = []\n",
    "\n",
    "    # \"predicted_pickup_values_list\"\n",
    "    # it is list of lists\n",
    "    predicted_pickup_values_list = []\n",
    "\n",
    "    predicted_value = -1  #it will contain cuurent predicted_value. Default is given -1 which will be replaced later\n",
    "\n",
    "    window_size = 2\n",
    "    for i in range(numberofstands):\n",
    "        for j in range(lengthoftimeseries):\n",
    "            if j == 0:\n",
    "                predicted_value = regionWisePickup_porto_data[ii][j]\n",
    "                predicted_pickup_values.append(0)\n",
    "            else:\n",
    "                if j>=window_size:\n",
    "                    sumPickups = 0\n",
    "                    sumOfWeights = 0\n",
    "                    for k in range(window_size, 0, -1):\n",
    "                        sumPickups += k*(regionWisePickup_porto_data[ii][j -window_size + (k - 1)])\n",
    "                        sumOfWeights += k\n",
    "                    predicted_value = int(sumPickups/sumOfWeights)\n",
    "                    predicted_pickup_values.append(predicted_value)\n",
    "                else:\n",
    "                    sumPickups = 0\n",
    "                    sumOfWeights = 0\n",
    "                    for k in range(j, 0, -1):\n",
    "                        sumPickups += k*regionWisePickup_porto_data[ii][k-1]\n",
    "                        sumOfWeights += k\n",
    "                    predicted_value = int(sumPickups/sumOfWeights)\n",
    "                    predicted_pickup_values.append(predicted_value)\n",
    "\n",
    "        predicted_pickup_values_list.append(predicted_pickup_values[10:])\n",
    "        predicted_pickup_values = []\n",
    "    trainsize =int((lengthoftimeseries - number_of_time_stamps)*numberofstands*0.7)\n",
    "    testsize=int((lengthoftimeseries - number_of_time_stamps)*numberofstands*0.3)\n",
    "    trainsizeonestand=int((lengthoftimeseries - number_of_time_stamps)*0.7)\n",
    "    testsizeonestand=int((lengthoftimeseries - number_of_time_stamps)*0.3)\n",
    "    train_previousFive_pickups  = [feat[(i*(lengthoftimeseries - number_of_time_stamps)):((lengthoftimeseries - number_of_time_stamps)*i+trainsizeonestand)] for i in range(numberofstands)]\n",
    "    test_previousFive_pickups  = [feat[(i*(lengthoftimeseries - number_of_time_stamps)+trainsizeonestand):((lengthoftimeseries - number_of_time_stamps)*(i+1))] for i in range(numberofstands)]\n",
    "    print(\"Train Data: Total number of clusters = {}. Number of points in each cluster = {}. Total number of training points = {}\".format(len(train_previousFive_pickups), len(train_previousFive_pickups[0]), len(train_previousFive_pickups)*len(train_previousFive_pickups[0])))\n",
    "    print(\"Test Data: Total number of clusters = {}. Number of points in each cluster = {}. Total number of test points = {}\".format(len(test_previousFive_pickups), len(test_previousFive_pickups[0]), len(test_previousFive_pickups)*len(test_previousFive_pickups[0])))\n",
    "    #taking 70% data as train data from each cluster\n",
    "    train_lat = [i[:trainsizeonestand] for i in lat]\n",
    "    train_lon = [i[:trainsizeonestand] for i in lon]\n",
    "    train_weekDay = [i[:trainsizeonestand] for i in day_of_week]\n",
    "    train_weighted_avg = [i[:trainsizeonestand] for i in predicted_pickup_values_list]\n",
    "    train_TruePickups = [i[:trainsizeonestand] for i in TruePickups]\n",
    "    #taking 30% data as test data from each cluster\n",
    "    test_lat = [i[trainsizeonestand:] for i in lat]\n",
    "    test_lon = [i[trainsizeonestand:] for i in lon]\n",
    "    test_weekDay = [i[trainsizeonestand:] for i in day_of_week]\n",
    "    test_weighted_avg = [i[trainsizeonestand:] for i in predicted_pickup_values_list]\n",
    "    test_TruePickups = [i[trainsizeonestand:] for i in TruePickups]\n",
    "    # convert from lists of lists of list to lists of list\n",
    "    train_pickups = []\n",
    "    test_pickups = []\n",
    "    for i in range(numberofstands):\n",
    "        train_pickups.extend(train_previousFive_pickups[i])\n",
    "        test_pickups.extend(test_previousFive_pickups[i])    \n",
    "    train_prevPickups_freq_amp = train_pickups\n",
    "    test_prevPickups_freq_amp = test_pickups\n",
    "    print(\"Number of data points in train data = {}. Number of columns till now = {}\".format(len(train_prevPickups_freq_amp), len(train_prevPickups_freq_amp[0])))\n",
    "    print(\"Number of data points in test data = {}. Number of columns till now = {}\".format(len(test_prevPickups_freq_amp), len(test_prevPickups_freq_amp[0])))\n",
    "    # converting lists of lists into single list i.e flatten\n",
    "    # a  = [[1,2,3,4],[4,6,7,8]]\n",
    "    # print(sum(a,[]))\n",
    "    # [1, 2, 3, 4, 4, 6, 7, 8]\n",
    "\n",
    "    train_flat_lat = sum(train_lat, [])\n",
    "    train_flat_lon = sum(train_lon, [])\n",
    "    train_flat_weekDay = sum(train_weekDay, [])\n",
    "    train_weighted_avg_flat = sum(train_weighted_avg, [])\n",
    "    train_TruePickups_flat = sum(train_TruePickups, [])\n",
    "\n",
    "    test_flat_lat = sum(test_lat, [])\n",
    "    test_flat_lon = sum(test_lon, [])\n",
    "    test_flat_weekDay = sum(test_weekDay, [])\n",
    "    test_weighted_avg_flat = sum(test_weighted_avg, [])\n",
    "    test_TruePickups_flat = sum(test_TruePickups, [])\n",
    "    columns=[]\n",
    "    for i in range(number_of_time_stamps):\n",
    "        ftname=\"ft\"+str(i)\n",
    "        columns.append(ftname)\n",
    "    #columns = ['ft_10','ft_9','ft_8','ft_7','ft_6','ft_5','ft_4','ft_3','ft_2','ft_1']\n",
    "    Train_DF = pd.DataFrame(data = train_prevPickups_freq_amp, columns = columns)\n",
    "    Test_DF = pd.DataFrame(data = test_prevPickups_freq_amp, columns = columns)\n",
    "\n",
    "    trainMAPE_lr, trainMSE_lr, train_RMSE_lr, train_SMAPE_lr,testMAPE_lr, testMSE_lr, test_RMSE_lr,test_SMAPE_lr,alpha_lr = lin_regression(Train_DF, train_TruePickups_flat, Test_DF, test_TruePickups_flat)\n",
    "    #rainMAPE_rf, trainMSE_rf, train_RMSE_rf, train_SMAPE_rf,testMAPE_rf, testMSE_rf, test_RMSE_rf,test_SMAPE_rf = randomFor(Train_DF, train_TruePickups_flat, Test_DF, test_TruePickups_flat)\n",
    "    #trainMAPE_xgb, trainMSE_xgb, train_RMSE_xgb, train_SMAPE_xgb,testMAPE_xgb, testMSE_xgb, test_RMSE_xgb,test_SMAPE_xgb = xgboost_reg(Train_DF, train_TruePickups_flat, Test_DF, test_TruePickups_flat)    \n",
    "\n",
    "    aalpha_lr.append(alpha_lr)\n",
    "    atrainMSE_lr.append(trainMSE_lr)\n",
    "    atestMSE_lr.append(testMSE_lr)\n",
    "    atrain_SMAPE_lr.append(train_SMAPE_lr)\n",
    "    atest_SMAPE_lr.append(test_SMAPE_lr)\n",
    "    \n",
    "print(\"Train MSE:\")\n",
    "for element in atrainMSE_lr:\n",
    "    print(element)\n",
    "print(\"Test MSE:\")\n",
    "for element in atestMSE_lr:\n",
    "    print(element)\n",
    "print(\"Train sMAPE:\")\n",
    "for element in atrain_SMAPE_lr:\n",
    "    print(element)\n",
    "print(\"Test sMAPE:\")\n",
    "for element in atest_SMAPE_lr:\n",
    "    print(element)\n",
    "print(\"Alpha:\")\n",
    "for element in aalpha_lr:\n",
    "    print(element)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numberoftaxistands = 63\n",
    "atrainMSE_rf = []\n",
    "atestMSE_rf= []\n",
    "atrain_SMAPE_rf =[]\n",
    "atest_SMAPE_rf = []\n",
    "atree_rf=[]\n",
    "for ii in range(numberoftaxistands):\n",
    "    print(\"Stand: \" + str(ii+1))\n",
    "    # TruePickups varaible\n",
    "    # it is list of lists\n",
    "    # It will be used as true labels/ground truth. Now since we are taking previous 10 pickups as a training data for predicting\n",
    "    # next pickup(here next pickup will be a true/ground truth pickup), so \"TruePickups\" will not contain first five pickups of each \n",
    "    # cluster. It will contain number of pickups 17519-10 = 17509 for each cluster. \n",
    "    TruePickups = []\n",
    "    #number_of_time_stamps = 15 #Linear\n",
    "    number_of_time_stamps = 12 #Random forest\n",
    "    #number_of_time_stamps = 19 #XGBoost\n",
    "    # lat,lon will contain 17519-10=17509 times latitude of cluster center for every stand\n",
    "    # it is list of lists\n",
    "    lat = []\n",
    "    lon = []\n",
    "    # we will code each day \n",
    "    # sunday = 0, monday=1, tue = 2, wed=3, thur=4, fri=5,sat=6\n",
    "    day_of_week = []\n",
    "\n",
    "    feat = []\n",
    "    numberofstands = 1 #Number of taxi stand\n",
    "    lengthoftimeseries= int(len(regionWisePickup_porto_data[0])/1)\n",
    "\n",
    "    feat = [0]*number_of_time_stamps\n",
    "    for i in range(numberofstands):\n",
    "            lat.append([centerOfRegions[ii][0]]*(lengthoftimeseries-number_of_time_stamps)) \n",
    "            lon.append([centerOfRegions[ii][1]]*(lengthoftimeseries-number_of_time_stamps))\n",
    "            day_of_week.append([int(((int(j/48)%7)+number_of_time_stamps)%7) for j in range(number_of_time_stamps, lengthoftimeseries)])\n",
    "            #48 is the time bin for a day (30 min)\n",
    "            feat = np.vstack((feat, [regionWisePickup_porto_data[ii][k:k+number_of_time_stamps] for k in range(0, len(regionWisePickup_porto_data[ii]) - (number_of_time_stamps))]))\n",
    "            #feat = np.vstack(([regionWisePickup_porto_data[i][k:k+number_of_time_stamps] for k in range(0, len(regionWisePickup_porto_data[i]) - (number_of_time_stamps))]))\n",
    "            TruePickups.append(regionWisePickup_porto_data[ii][number_of_time_stamps:])\n",
    "            #output contains pickup values of all the regions and of each time stamp, except first 5 time stamp pickups of each region.\n",
    "    feat = feat[1:]\n",
    "    # \"predicted_pickup_values\": it is a temporary array that store weighted moving avarag prediction values for each 30min intervl, \n",
    "    # for each cluster it will get reset.\n",
    "    # for every cluster it contains 17519 values\n",
    "    predicted_pickup_values = []\n",
    "\n",
    "    # \"predicted_pickup_values_list\"\n",
    "    # it is list of lists\n",
    "    predicted_pickup_values_list = []\n",
    "\n",
    "    predicted_value = -1  #it will contain cuurent predicted_value. Default is given -1 which will be replaced later\n",
    "\n",
    "    window_size = 2\n",
    "    for i in range(numberofstands):\n",
    "        for j in range(lengthoftimeseries):\n",
    "            if j == 0:\n",
    "                predicted_value = regionWisePickup_porto_data[ii][j]\n",
    "                predicted_pickup_values.append(0)\n",
    "            else:\n",
    "                if j>=window_size:\n",
    "                    sumPickups = 0\n",
    "                    sumOfWeights = 0\n",
    "                    for k in range(window_size, 0, -1):\n",
    "                        sumPickups += k*(regionWisePickup_porto_data[ii][j -window_size + (k - 1)])\n",
    "                        sumOfWeights += k\n",
    "                    predicted_value = int(sumPickups/sumOfWeights)\n",
    "                    predicted_pickup_values.append(predicted_value)\n",
    "                else:\n",
    "                    sumPickups = 0\n",
    "                    sumOfWeights = 0\n",
    "                    for k in range(j, 0, -1):\n",
    "                        sumPickups += k*regionWisePickup_porto_data[ii][k-1]\n",
    "                        sumOfWeights += k\n",
    "                    predicted_value = int(sumPickups/sumOfWeights)\n",
    "                    predicted_pickup_values.append(predicted_value)\n",
    "\n",
    "        predicted_pickup_values_list.append(predicted_pickup_values[10:])\n",
    "        predicted_pickup_values = []\n",
    "    trainsize =int((lengthoftimeseries - number_of_time_stamps)*numberofstands*0.7)\n",
    "    testsize=int((lengthoftimeseries - number_of_time_stamps)*numberofstands*0.3)\n",
    "    trainsizeonestand=int((lengthoftimeseries - number_of_time_stamps)*0.7)\n",
    "    testsizeonestand=int((lengthoftimeseries - number_of_time_stamps)*0.3)\n",
    "    train_previousFive_pickups  = [feat[(i*(lengthoftimeseries - number_of_time_stamps)):((lengthoftimeseries - number_of_time_stamps)*i+trainsizeonestand)] for i in range(numberofstands)]\n",
    "    test_previousFive_pickups  = [feat[(i*(lengthoftimeseries - number_of_time_stamps)+trainsizeonestand):((lengthoftimeseries - number_of_time_stamps)*(i+1))] for i in range(numberofstands)]\n",
    "    print(\"Train Data: Total number of clusters = {}. Number of points in each cluster = {}. Total number of training points = {}\".format(len(train_previousFive_pickups), len(train_previousFive_pickups[0]), len(train_previousFive_pickups)*len(train_previousFive_pickups[0])))\n",
    "    print(\"Test Data: Total number of clusters = {}. Number of points in each cluster = {}. Total number of test points = {}\".format(len(test_previousFive_pickups), len(test_previousFive_pickups[0]), len(test_previousFive_pickups)*len(test_previousFive_pickups[0])))\n",
    "    #taking 70% data as train data from each cluster\n",
    "    train_lat = [i[:trainsizeonestand] for i in lat]\n",
    "    train_lon = [i[:trainsizeonestand] for i in lon]\n",
    "    train_weekDay = [i[:trainsizeonestand] for i in day_of_week]\n",
    "    train_weighted_avg = [i[:trainsizeonestand] for i in predicted_pickup_values_list]\n",
    "    train_TruePickups = [i[:trainsizeonestand] for i in TruePickups]\n",
    "    #taking 30% data as test data from each cluster\n",
    "    test_lat = [i[trainsizeonestand:] for i in lat]\n",
    "    test_lon = [i[trainsizeonestand:] for i in lon]\n",
    "    test_weekDay = [i[trainsizeonestand:] for i in day_of_week]\n",
    "    test_weighted_avg = [i[trainsizeonestand:] for i in predicted_pickup_values_list]\n",
    "    test_TruePickups = [i[trainsizeonestand:] for i in TruePickups]\n",
    "    # convert from lists of lists of list to lists of list\n",
    "    train_pickups = []\n",
    "    test_pickups = []\n",
    "    for i in range(numberofstands):\n",
    "        train_pickups.extend(train_previousFive_pickups[i])\n",
    "        test_pickups.extend(test_previousFive_pickups[i])    \n",
    "    train_prevPickups_freq_amp = train_pickups\n",
    "    test_prevPickups_freq_amp = test_pickups\n",
    "    print(\"Number of data points in train data = {}. Number of columns till now = {}\".format(len(train_prevPickups_freq_amp), len(train_prevPickups_freq_amp[0])))\n",
    "    print(\"Number of data points in test data = {}. Number of columns till now = {}\".format(len(test_prevPickups_freq_amp), len(test_prevPickups_freq_amp[0])))\n",
    "    # converting lists of lists into single list i.e flatten\n",
    "    # a  = [[1,2,3,4],[4,6,7,8]]\n",
    "    # print(sum(a,[]))\n",
    "    # [1, 2, 3, 4, 4, 6, 7, 8]\n",
    "\n",
    "    train_flat_lat = sum(train_lat, [])\n",
    "    train_flat_lon = sum(train_lon, [])\n",
    "    train_flat_weekDay = sum(train_weekDay, [])\n",
    "    train_weighted_avg_flat = sum(train_weighted_avg, [])\n",
    "    train_TruePickups_flat = sum(train_TruePickups, [])\n",
    "\n",
    "    test_flat_lat = sum(test_lat, [])\n",
    "    test_flat_lon = sum(test_lon, [])\n",
    "    test_flat_weekDay = sum(test_weekDay, [])\n",
    "    test_weighted_avg_flat = sum(test_weighted_avg, [])\n",
    "    test_TruePickups_flat = sum(test_TruePickups, [])\n",
    "    columns=[]\n",
    "    for i in range(number_of_time_stamps):\n",
    "        ftname=\"ft\"+str(i)\n",
    "        columns.append(ftname)\n",
    "    #columns = ['ft_10','ft_9','ft_8','ft_7','ft_6','ft_5','ft_4','ft_3','ft_2','ft_1']\n",
    "    Train_DF = pd.DataFrame(data = train_prevPickups_freq_amp, columns = columns)\n",
    "    Test_DF = pd.DataFrame(data = test_prevPickups_freq_amp, columns = columns)\n",
    "\n",
    "    #trainMAPE_lr, trainMSE_lr, train_RMSE_lr, train_SMAPE_lr,testMAPE_lr, testMSE_lr, test_RMSE_lr,test_SMAPE_lr = lin_regression(Train_DF, train_TruePickups_flat, Test_DF, test_TruePickups_flat)\n",
    "    rainMAPE_rf, trainMSE_rf, train_RMSE_rf, train_SMAPE_rf,testMAPE_rf, testMSE_rf, test_RMSE_rf,test_SMAPE_rf, params = randomFor(Train_DF, train_TruePickups_flat, Test_DF, test_TruePickups_flat)\n",
    "    #trainMAPE_xgb, trainMSE_xgb, train_RMSE_xgb, train_SMAPE_xgb,testMAPE_xgb, testMSE_xgb, test_RMSE_xgb,test_SMAPE_xgb = xgboost_reg(Train_DF, train_TruePickups_flat, Test_DF, test_TruePickups_flat)    \n",
    "\n",
    "    atree_rf.append(params)\n",
    "    atrainMSE_rf.append(trainMSE_rf)\n",
    "    atestMSE_rf.append(testMSE_rf)\n",
    "    atrain_SMAPE_rf.append(train_SMAPE_rf)\n",
    "    atest_SMAPE_rf.append(test_SMAPE_rf)\n",
    "    \n",
    "print(\"Train MSE:\")\n",
    "for element in atrainMSE_rf:\n",
    "    print(element)\n",
    "print(\"Test MSE:\")\n",
    "for element in atestMSE_rf:\n",
    "    print(element)\n",
    "print(\"Train sMAPE:\")\n",
    "for element in atrain_SMAPE_rf:\n",
    "    print(element)\n",
    "print(\"Test sMAPE:\")\n",
    "for element in atest_SMAPE_rf:\n",
    "    print(element)\n",
    "print(\"Number of tree:\")\n",
    "for element in atree_rf:\n",
    "    print(element)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numberoftaxistands = 63\n",
    "atrainMSE_xgb = []\n",
    "atestMSE_xgb= []\n",
    "atrain_SMAPE_xgb =[]\n",
    "atest_SMAPE_xgb = []\n",
    "adepth_xgb=[]\n",
    "atree_xgb=[]\n",
    "for ii in range(numberoftaxistands):\n",
    "    print(\"Stand: \" + str(ii+1))\n",
    "    # TruePickups varaible\n",
    "    # it is list of lists\n",
    "    # It will be used as true labels/ground truth. Now since we are taking previous 10 pickups as a training data for predicting\n",
    "    # next pickup(here next pickup will be a true/ground truth pickup), so \"TruePickups\" will not contain first five pickups of each \n",
    "    # cluster. It will contain number of pickups 17519-10 = 17509 for each cluster. \n",
    "    TruePickups = []\n",
    "    #number_of_time_stamps = 15 #Linear\n",
    "    #number_of_time_stamps = 12 #Random forest\n",
    "    number_of_time_stamps = 19 #XGBoost\n",
    "    # lat,lon will contain 17519-10=17509 times latitude of cluster center for every stand\n",
    "    # it is list of lists\n",
    "    lat = []\n",
    "    lon = []\n",
    "    # we will code each day \n",
    "    # sunday = 0, monday=1, tue = 2, wed=3, thur=4, fri=5,sat=6\n",
    "    day_of_week = []\n",
    "\n",
    "    feat = []\n",
    "    numberofstands = 1 #Number of taxi stand\n",
    "    lengthoftimeseries= int(len(regionWisePickup_porto_data[0])/1)\n",
    "\n",
    "    feat = [0]*number_of_time_stamps\n",
    "    for i in range(numberofstands):\n",
    "            lat.append([centerOfRegions[ii][0]]*(lengthoftimeseries-number_of_time_stamps)) \n",
    "            lon.append([centerOfRegions[ii][1]]*(lengthoftimeseries-number_of_time_stamps))\n",
    "            day_of_week.append([int(((int(j/48)%7)+number_of_time_stamps)%7) for j in range(number_of_time_stamps, lengthoftimeseries)])\n",
    "            #48 is the time bin for a day (30 min)\n",
    "            feat = np.vstack((feat, [regionWisePickup_porto_data[ii][k:k+number_of_time_stamps] for k in range(0, len(regionWisePickup_porto_data[ii]) - (number_of_time_stamps))]))\n",
    "            #feat = np.vstack(([regionWisePickup_porto_data[i][k:k+number_of_time_stamps] for k in range(0, len(regionWisePickup_porto_data[i]) - (number_of_time_stamps))]))\n",
    "            TruePickups.append(regionWisePickup_porto_data[ii][number_of_time_stamps:])\n",
    "            #output contains pickup values of all the regions and of each time stamp, except first 5 time stamp pickups of each region.\n",
    "    feat = feat[1:]\n",
    "    # \"predicted_pickup_values\": it is a temporary array that store weighted moving avarag prediction values for each 30min intervl, \n",
    "    # for each cluster it will get reset.\n",
    "    # for every cluster it contains 17519 values\n",
    "    predicted_pickup_values = []\n",
    "\n",
    "    # \"predicted_pickup_values_list\"\n",
    "    # it is list of lists\n",
    "    predicted_pickup_values_list = []\n",
    "\n",
    "    predicted_value = -1  #it will contain cuurent predicted_value. Default is given -1 which will be replaced later\n",
    "\n",
    "    window_size = 2\n",
    "    for i in range(numberofstands):\n",
    "        for j in range(lengthoftimeseries):\n",
    "            if j == 0:\n",
    "                predicted_value = regionWisePickup_porto_data[ii][j]\n",
    "                predicted_pickup_values.append(0)\n",
    "            else:\n",
    "                if j>=window_size:\n",
    "                    sumPickups = 0\n",
    "                    sumOfWeights = 0\n",
    "                    for k in range(window_size, 0, -1):\n",
    "                        sumPickups += k*(regionWisePickup_porto_data[ii][j -window_size + (k - 1)])\n",
    "                        sumOfWeights += k\n",
    "                    predicted_value = int(sumPickups/sumOfWeights)\n",
    "                    predicted_pickup_values.append(predicted_value)\n",
    "                else:\n",
    "                    sumPickups = 0\n",
    "                    sumOfWeights = 0\n",
    "                    for k in range(j, 0, -1):\n",
    "                        sumPickups += k*regionWisePickup_porto_data[ii][k-1]\n",
    "                        sumOfWeights += k\n",
    "                    predicted_value = int(sumPickups/sumOfWeights)\n",
    "                    predicted_pickup_values.append(predicted_value)\n",
    "\n",
    "        predicted_pickup_values_list.append(predicted_pickup_values[10:])\n",
    "        predicted_pickup_values = []\n",
    "    trainsize =int((lengthoftimeseries - number_of_time_stamps)*numberofstands*0.7)\n",
    "    testsize=int((lengthoftimeseries - number_of_time_stamps)*numberofstands*0.3)\n",
    "    trainsizeonestand=int((lengthoftimeseries - number_of_time_stamps)*0.7)\n",
    "    testsizeonestand=int((lengthoftimeseries - number_of_time_stamps)*0.3)\n",
    "    train_previousFive_pickups  = [feat[(i*(lengthoftimeseries - number_of_time_stamps)):((lengthoftimeseries - number_of_time_stamps)*i+trainsizeonestand)] for i in range(numberofstands)]\n",
    "    test_previousFive_pickups  = [feat[(i*(lengthoftimeseries - number_of_time_stamps)+trainsizeonestand):((lengthoftimeseries - number_of_time_stamps)*(i+1))] for i in range(numberofstands)]\n",
    "    print(\"Train Data: Total number of clusters = {}. Number of points in each cluster = {}. Total number of training points = {}\".format(len(train_previousFive_pickups), len(train_previousFive_pickups[0]), len(train_previousFive_pickups)*len(train_previousFive_pickups[0])))\n",
    "    print(\"Test Data: Total number of clusters = {}. Number of points in each cluster = {}. Total number of test points = {}\".format(len(test_previousFive_pickups), len(test_previousFive_pickups[0]), len(test_previousFive_pickups)*len(test_previousFive_pickups[0])))\n",
    "    #taking 70% data as train data from each cluster\n",
    "    train_lat = [i[:trainsizeonestand] for i in lat]\n",
    "    train_lon = [i[:trainsizeonestand] for i in lon]\n",
    "    train_weekDay = [i[:trainsizeonestand] for i in day_of_week]\n",
    "    train_weighted_avg = [i[:trainsizeonestand] for i in predicted_pickup_values_list]\n",
    "    train_TruePickups = [i[:trainsizeonestand] for i in TruePickups]\n",
    "    #taking 30% data as test data from each cluster\n",
    "    test_lat = [i[trainsizeonestand:] for i in lat]\n",
    "    test_lon = [i[trainsizeonestand:] for i in lon]\n",
    "    test_weekDay = [i[trainsizeonestand:] for i in day_of_week]\n",
    "    test_weighted_avg = [i[trainsizeonestand:] for i in predicted_pickup_values_list]\n",
    "    test_TruePickups = [i[trainsizeonestand:] for i in TruePickups]\n",
    "    # convert from lists of lists of list to lists of list\n",
    "    train_pickups = []\n",
    "    test_pickups = []\n",
    "    for i in range(numberofstands):\n",
    "        train_pickups.extend(train_previousFive_pickups[i])\n",
    "        test_pickups.extend(test_previousFive_pickups[i])    \n",
    "    train_prevPickups_freq_amp = train_pickups\n",
    "    test_prevPickups_freq_amp = test_pickups\n",
    "    print(\"Number of data points in train data = {}. Number of columns till now = {}\".format(len(train_prevPickups_freq_amp), len(train_prevPickups_freq_amp[0])))\n",
    "    print(\"Number of data points in test data = {}. Number of columns till now = {}\".format(len(test_prevPickups_freq_amp), len(test_prevPickups_freq_amp[0])))\n",
    "    # converting lists of lists into single list i.e flatten\n",
    "    # a  = [[1,2,3,4],[4,6,7,8]]\n",
    "    # print(sum(a,[]))\n",
    "    # [1, 2, 3, 4, 4, 6, 7, 8]\n",
    "\n",
    "    train_flat_lat = sum(train_lat, [])\n",
    "    train_flat_lon = sum(train_lon, [])\n",
    "    train_flat_weekDay = sum(train_weekDay, [])\n",
    "    train_weighted_avg_flat = sum(train_weighted_avg, [])\n",
    "    train_TruePickups_flat = sum(train_TruePickups, [])\n",
    "\n",
    "    test_flat_lat = sum(test_lat, [])\n",
    "    test_flat_lon = sum(test_lon, [])\n",
    "    test_flat_weekDay = sum(test_weekDay, [])\n",
    "    test_weighted_avg_flat = sum(test_weighted_avg, [])\n",
    "    test_TruePickups_flat = sum(test_TruePickups, [])\n",
    "    columns=[]\n",
    "    for i in range(number_of_time_stamps):\n",
    "        ftname=\"ft\"+str(i)\n",
    "        columns.append(ftname)\n",
    "    #columns = ['ft_10','ft_9','ft_8','ft_7','ft_6','ft_5','ft_4','ft_3','ft_2','ft_1']\n",
    "    Train_DF = pd.DataFrame(data = train_prevPickups_freq_amp, columns = columns)\n",
    "    Test_DF = pd.DataFrame(data = test_prevPickups_freq_amp, columns = columns)\n",
    "\n",
    "    #trainMAPE_lr, trainMSE_lr, train_RMSE_lr, train_SMAPE_lr,testMAPE_lr, testMSE_lr, test_RMSE_lr,test_SMAPE_lr = lin_regression(Train_DF, train_TruePickups_flat, Test_DF, test_TruePickups_flat)\n",
    "    #rainMAPE_rf, trainMSE_rf, train_RMSE_rf, train_SMAPE_rf,testMAPE_rf, testMSE_rf, test_RMSE_rf,test_SMAPE_rf = randomFor(Train_DF, train_TruePickups_flat, Test_DF, test_TruePickups_flat)\n",
    "    trainMAPE_xgb, trainMSE_xgb, train_RMSE_xgb, train_SMAPE_xgb,testMAPE_xgb, testMSE_xgb, test_RMSE_xgb,test_SMAPE_xgb,depth_xgb,tree_xgb = xgboost_reg(Train_DF, train_TruePickups_flat, Test_DF, test_TruePickups_flat)    \n",
    "\n",
    "    adepth_xgb.append(depth_xgb)\n",
    "    atree_xgb.append(tree_xgb)\n",
    "    atrainMSE_xgb.append(trainMSE_xgb)\n",
    "    atestMSE_xgb.append(testMSE_xgb)\n",
    "    atrain_SMAPE_xgb.append(train_SMAPE_xgb)\n",
    "    atest_SMAPE_xgb.append(test_SMAPE_xgb)\n",
    "    \n",
    "print(\"Train MSE:\")\n",
    "for element in atrainMSE_xgb:\n",
    "    print(element)\n",
    "print(\"Test MSE:\")\n",
    "for element in atestMSE_xgb:\n",
    "    print(element)\n",
    "print(\"Train sMAPE:\")\n",
    "for element in atrain_SMAPE_xgb:\n",
    "    print(element)\n",
    "print(\"Test sMAPE:\")\n",
    "for element in atest_SMAPE_xgb:\n",
    "    print(element)\n",
    "print(\"Depth:\")\n",
    "for element in adepth_xgb:\n",
    "    print(element)\n",
    "print(\"Tree:\")\n",
    "for element in atree_xgb:\n",
    "    print(element)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple and traditional models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Moving Average Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def movingaverage(values, window):\n",
    "    weights = np.repeat(1.0, window)/window\n",
    "    predict = np.convolve(values, weights,'valid')\n",
    "    s_mape = smape(values[window-1:],predict)\n",
    "    mse = mean_squared_error(values[window-1:],predict)\n",
    "    return mse,s_mape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def moving_average_prediction(dataframe,numberofcluster,window):\n",
    "    mse=[]\n",
    "    s_mape=[]        \n",
    "    for i in range(numberofcluster):\n",
    "        #if (i in count_points_new): ## after adjusted\n",
    "            dt=dataframe[i]\n",
    "            df = pd.DataFrame({'pickup':dt})\n",
    "            dataset=df.values        \n",
    "            m,s = movingaverage(dataset[:,0], window)\n",
    "            mse.append(m)\n",
    "            s_mape.append(s)\n",
    "    mean_mse = np.mean(mse)\n",
    "    mean_smape = np.mean(s_mape)\n",
    "    return mse,s_mape        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MSE_ma,SMAPE_ma = moving_average_prediction(regionWisePickup_porto_data,63,20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define SMAPE\n",
    "#{\\displaystyle {\\text{SMAPE}}={\\frac {100\\%}{n}}\\sum _{t=1}^{n}{\\frac {|F_{t}-A_{t}|}{|A_{t}|+|F_{t}|+c}}}\n",
    "#in our case c=1, use laplace correction\n",
    "def smape(y_true, y_pred):\n",
    "    denominator = (np.abs(y_true) + np.abs(y_pred)+1) / 100.0\n",
    "    diff = np.abs(y_true - y_pred) / denominator\n",
    "    #diff[denominator == 0] = 1\n",
    "    return np.nanmean(diff)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert an array of values into a dataset matrix\n",
    "def create_dataset(dataset, look_back=1):\n",
    "    dataX, dataY = [], []\n",
    "    for i in range(len(dataset)-look_back-1):\n",
    "        a = dataset[i:(i+look_back), 0]\n",
    "        dataX.append(a)\n",
    "        dataY.append(dataset[i + look_back, 0])\n",
    "    return np.array(dataX), np.array(dataY)\n",
    "\n",
    "\n",
    "# fix random seed for reproducibility\n",
    "np.random.seed(7)\n",
    "#Memory\n",
    "import psutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import operator\n",
    "from pandas import concat\n",
    "from numpy import concatenate\n",
    "\n",
    "#Find the closet neighbour\n",
    "def getNeighbors(k):\n",
    "    k_neighbour=[]\n",
    "    for i in range(63):        \n",
    "        k_neighbour.append([i])\n",
    "    #find    \n",
    "    for i in range(63):\n",
    "        #if (i in count_points_new):\n",
    "            distances = []\n",
    "            for j in range(63):\n",
    "                if (i!=j):#and (j in count_points_new):\n",
    "                    dist=gpxpy.geo.haversine_distance(centroids[i][0],centroids[i][1],centroids[j][0],centroids[j][1])\n",
    "                    distances.append((j, dist))\n",
    "                    distances.sort(key=operator.itemgetter(1))\n",
    "            for j in range(k):\n",
    "                k_neighbour[i].append(distances[j][0])\n",
    "        \n",
    "    return k_neighbour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_neighbour = getNeighbors(62)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert series to supervised learning\n",
    "def series_to_supervised(data, n_in=1, n_out=1, dropnan=True):\n",
    "    n_vars = 1 if type(data) is list else data.shape[1]\n",
    "    df = pd.DataFrame(data)\n",
    "    cols, names = list(), list()\n",
    "    # input sequence (t-n, ... t-1)\n",
    "    for i in range(n_in, 0, -1):\n",
    "        cols.append(df.shift(i))\n",
    "        names += [('var%d(t-%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "    # forecast sequence (t, t+1, ... t+n)\n",
    "    for i in range(0, n_out):\n",
    "        cols.append(df.shift(-i))\n",
    "        if i == 0:\n",
    "            names += [('var%d(t)' % (j+1)) for j in range(n_vars)]\n",
    "        else:\n",
    "            names += [('var%d(t+%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "    # put it all together\n",
    "    agg = concat(cols, axis=1)\n",
    "    agg.columns = names\n",
    "    # drop rows with NaN values\n",
    "    if dropnan:\n",
    "        agg.dropna(inplace=True)\n",
    "    return agg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM turning parameter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Turning look_back value "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "best_look_back=2\n",
    "min_sMAPE_lstm = 100\n",
    "\n",
    "k=0\n",
    "for look_back in range(2,20):    \n",
    "        \n",
    "        #Dataset for LSTM\n",
    "        dt=np.array([[j] for j in regionWisePickup_porto_data[0]])\n",
    "        #Take neighbour\n",
    "        #for h in range(k):\n",
    "        #    dt1=np.array([[j] for j in regionWisePickup_porto_data[k_neighbour[i][h+1]]])\n",
    "        #    dt=np.concatenate((dt,dt1),axis=1)\n",
    "        #Generate a time series with count of POI for stand\n",
    "        #standpoi=np.full((len(regionWisePickup_porto_data[i]), 1), poi[i])\n",
    "        #dt=np.concatenate((dt,standpoi),axis=1)\n",
    "        # ensure all data is float\n",
    "        values = dt.astype('float32')\n",
    "        # normalize features\n",
    "        scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "        scaled = scaler.fit_transform(values)\n",
    "        #look_back = 10 #number of data look back - to predict, default 1\n",
    "        # frame as supervised learning\n",
    "        reframed = series_to_supervised(scaled, look_back, 1)\n",
    "        # split into train and test sets\n",
    "        values = reframed.values\n",
    "        train_size = int(len(values) * 0.7)\n",
    "        test_size = len(values) - train_size\n",
    "        #n_train_hours\n",
    "        train = values[:train_size, :]\n",
    "        test = values[train_size:, :]\n",
    "        # split into input and outputs\n",
    "        n_features = k + 1 # include itself and one extended feature\n",
    "        #n_features = k + 1\n",
    "        n_obs = look_back * n_features\n",
    "        train_X, train_y = train[:, :n_obs], train[:, -n_features]\n",
    "        test_X, test_y = test[:, :n_obs], test[:, -n_features]\n",
    "        #print(train_X.shape, len(train_X), train_y.shape)\n",
    "        # reshape input to be 3D [samples, timesteps, features]\n",
    "        train_X = train_X.reshape((train_X.shape[0], look_back, n_features))\n",
    "        test_X = test_X.reshape((test_X.shape[0], look_back, n_features))\n",
    "        #print(train_X.shape, train_y.shape, test_X.shape, test_y.shape)\n",
    "        # design network\n",
    "        model = Sequential()\n",
    "        model.add(LSTM(50, input_shape=(train_X.shape[1], train_X.shape[2])))\n",
    "        #model.add(LSTM(50, input_shape=(train_X.shape[1], train_X.shape[2]),return_sequences=True))\n",
    "        #model.add(LSTM(100,return_sequences=True))\n",
    "        #model.add(LSTM(50,return_sequences=True))\n",
    "        #model.add(LSTM(100))\n",
    "        model.add(Dense(1))\n",
    "        model.compile(loss='mae', optimizer='adam')\n",
    "        model.summary()\n",
    "        # fit network\n",
    "        history = model.fit(train_X, train_y, epochs=10, batch_size=72, validation_data=(test_X, test_y), verbose=2, shuffle=False)\n",
    "        # plot history\n",
    "        #plt.plot(history.history['loss'], label='train')\n",
    "        #plt.plot(history.history['val_loss'], label='test')\n",
    "        #plt.legend()\n",
    "        #plt.show()\n",
    "        #figname=\"Plot\"+str(i)+\".png\"\n",
    "        #plt.savefig(figname,formatt=\"png\")\n",
    "        \n",
    "        # make a prediction of train set\n",
    "        ytrainpredict = model.predict(train_X)\n",
    "        train_X = train_X.reshape((train_X.shape[0], look_back*n_features))\n",
    "        inv_ytrainpredict = concatenate((ytrainpredict, train_X[:, -(n_features-1):]), axis=1)\n",
    "        inv_ytrainpredict = scaler.inverse_transform(inv_ytrainpredict)\n",
    "        inv_ytrainpredict = inv_ytrainpredict[:,0]\n",
    "        train_y = train_y.reshape((len(train_y), 1))\n",
    "        inv_trainy = concatenate((train_y, train_X[:, -(n_features-1):]), axis=1)\n",
    "        inv_trainy = scaler.inverse_transform(inv_trainy)\n",
    "        inv_trainy = inv_trainy[:,0]\n",
    "        #calculate score\n",
    "        trainMSE = mean_squared_error(inv_trainy, inv_ytrainpredict)\n",
    "        \n",
    "        trainRMSE = math.sqrt(trainMSE)\n",
    "        \n",
    "        train_MAPE = mean_absolute_error(inv_trainy, inv_ytrainpredict)/ (sum(inv_trainy)/len(inv_trainy)) * 100\n",
    "        \n",
    "        train_SMAPE = smape(inv_trainy, inv_ytrainpredict)\n",
    "        \n",
    "        #print('Train MSE: %.3f' %trainMSE)\n",
    "        #print('Train RMSE: %.3f' %trainRMSE)\n",
    "        #print('Train MAPE: %.3f' %train_MAPE)\n",
    "        #print('Train SMAPE: %.3f' %train_SMAPE)\n",
    "        \n",
    "        \n",
    "        # make a prediction of test set\n",
    "        ytestpredtict = model.predict(test_X)        \n",
    "        test_X = test_X.reshape((test_X.shape[0], look_back*n_features))\n",
    "        # invert scaling for forecast\n",
    "        inv_ytestpredict = concatenate((ytestpredtict, test_X[:, -(n_features-1):]), axis=1)\n",
    "        inv_ytestpredict = scaler.inverse_transform(inv_ytestpredict)\n",
    "        inv_ytestpredict = inv_ytestpredict[:,0]\n",
    "        # invert scaling for actual\n",
    "        test_y = test_y.reshape((len(test_y), 1))\n",
    "        inv_y = concatenate((test_y, test_X[:, -(n_features-1):]), axis=1)\n",
    "        inv_y = scaler.inverse_transform(inv_y)\n",
    "        inv_y = inv_y[:,0]\n",
    "        # calculate score\n",
    "        testMSE = mean_squared_error(inv_y, inv_ytestpredict)\n",
    "        \n",
    "        testRMSE = math.sqrt(testMSE)\n",
    "        \n",
    "        test_MAPE = mean_absolute_error(inv_y, inv_ytestpredict)/ (sum(inv_y)/len(inv_y)) * 100\n",
    "        \n",
    "        test_SMAPE = smape(inv_y, inv_ytestpredict)\n",
    "        \n",
    "        #rmse = math.sqrt(mean_squared_error(inv_y, inv_yhat))\n",
    "        #print('Test MSE: %.3f' % testMSE)\n",
    "        #print('Test RMSE: %.3f' % testRMSE)  \n",
    "        #print('Test MAPE: %.3f' % test_MAPE)\n",
    "        print('Look back value: ' + str(look_back))    \n",
    "        print('Test SMAPE: %.3f' % test_SMAPE)   \n",
    "        if (test_SMAPE<min_sMAPE_lstm):\n",
    "            min_sMAPE_lstm=test_SMAPE\n",
    "            best_look_back=look_back\n",
    "print('The best look back value: ' + str(best_look_back)+ \", sMAPE: \" + str(min_sMAPE_lstm))    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Optimizer Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer_al = ['SGD', 'RMSprop', 'Adagrad', 'Adadelta', 'Adam', 'Adamax', 'Nadam']\n",
    "best_optimizer=0\n",
    "min_sMAPE_lstm = 100\n",
    "\n",
    "k=0\n",
    "for opt in range(len(optimizer_al)):    \n",
    "        \n",
    "        #Dataset for LSTM\n",
    "        dt=np.array([[j] for j in regionWisePickup_porto_data[0]])\n",
    "        #Take neighbour\n",
    "        #for h in range(k):\n",
    "        #    dt1=np.array([[j] for j in regionWisePickup_porto_data[k_neighbour[i][h+1]]])\n",
    "        #    dt=np.concatenate((dt,dt1),axis=1)\n",
    "        #Generate a time series with count of POI for stand\n",
    "        #standpoi=np.full((len(regionWisePickup_porto_data[i]), 1), poi[i])\n",
    "        #dt=np.concatenate((dt,standpoi),axis=1)\n",
    "        # ensure all data is float\n",
    "        values = dt.astype('float32')\n",
    "        # normalize features\n",
    "        scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "        scaled = scaler.fit_transform(values)\n",
    "        #look_back = 10 #number of data look back - to predict, default 1\n",
    "        # frame as supervised learning\n",
    "        reframed = series_to_supervised(scaled, look_back, 1)\n",
    "        # split into train and test sets\n",
    "        values = reframed.values\n",
    "        train_size = int(len(values) * 0.7)\n",
    "        test_size = len(values) - train_size\n",
    "        #n_train_hours\n",
    "        train = values[:train_size, :]\n",
    "        test = values[train_size:, :]\n",
    "        # split into input and outputs\n",
    "        n_features = k + 1 # include itself and one extended feature\n",
    "        #n_features = k + 1\n",
    "        n_obs = look_back * n_features\n",
    "        train_X, train_y = train[:, :n_obs], train[:, -n_features]\n",
    "        test_X, test_y = test[:, :n_obs], test[:, -n_features]\n",
    "        #print(train_X.shape, len(train_X), train_y.shape)\n",
    "        # reshape input to be 3D [samples, timesteps, features]\n",
    "        train_X = train_X.reshape((train_X.shape[0], look_back, n_features))\n",
    "        test_X = test_X.reshape((test_X.shape[0], look_back, n_features))\n",
    "        #print(train_X.shape, train_y.shape, test_X.shape, test_y.shape)\n",
    "        # design network\n",
    "        model = Sequential()\n",
    "        model.add(LSTM(50, input_shape=(train_X.shape[1], train_X.shape[2])))\n",
    "        #model.add(LSTM(50, input_shape=(train_X.shape[1], train_X.shape[2]),return_sequences=True))\n",
    "        #model.add(LSTM(100,return_sequences=True))\n",
    "        #model.add(LSTM(50,return_sequences=True))\n",
    "        #model.add(LSTM(100))\n",
    "        model.add(Dense(1))\n",
    "        model.compile(loss='mae', optimizer=optimizer_al[opt])\n",
    "        model.summary()\n",
    "        # fit network\n",
    "        history = model.fit(train_X, train_y, epochs=10, batch_size=72, validation_data=(test_X, test_y), verbose=2, shuffle=False)\n",
    "        # plot history\n",
    "        #plt.plot(history.history['loss'], label='train')\n",
    "        #plt.plot(history.history['val_loss'], label='test')\n",
    "        #plt.legend()\n",
    "        #plt.show()\n",
    "        #figname=\"Plot\"+str(i)+\".png\"\n",
    "        #plt.savefig(figname,formatt=\"png\")\n",
    "        \n",
    "        # make a prediction of train set\n",
    "        ytrainpredict = model.predict(train_X)\n",
    "        train_X = train_X.reshape((train_X.shape[0], look_back*n_features))\n",
    "        inv_ytrainpredict = concatenate((ytrainpredict, train_X[:, -(n_features-1):]), axis=1)\n",
    "        inv_ytrainpredict = scaler.inverse_transform(inv_ytrainpredict)\n",
    "        inv_ytrainpredict = inv_ytrainpredict[:,0]\n",
    "        train_y = train_y.reshape((len(train_y), 1))\n",
    "        inv_trainy = concatenate((train_y, train_X[:, -(n_features-1):]), axis=1)\n",
    "        inv_trainy = scaler.inverse_transform(inv_trainy)\n",
    "        inv_trainy = inv_trainy[:,0]\n",
    "        #calculate score\n",
    "        trainMSE = mean_squared_error(inv_trainy, inv_ytrainpredict)\n",
    "        \n",
    "        trainRMSE = math.sqrt(trainMSE)\n",
    "        \n",
    "        train_MAPE = mean_absolute_error(inv_trainy, inv_ytrainpredict)/ (sum(inv_trainy)/len(inv_trainy)) * 100\n",
    "        \n",
    "        train_SMAPE = smape(inv_trainy, inv_ytrainpredict)\n",
    "        \n",
    "        #print('Train MSE: %.3f' %trainMSE)\n",
    "        #print('Train RMSE: %.3f' %trainRMSE)\n",
    "        #print('Train MAPE: %.3f' %train_MAPE)\n",
    "        #print('Train SMAPE: %.3f' %train_SMAPE)\n",
    "        \n",
    "        \n",
    "        # make a prediction of test set\n",
    "        ytestpredtict = model.predict(test_X)        \n",
    "        test_X = test_X.reshape((test_X.shape[0], look_back*n_features))\n",
    "        # invert scaling for forecast\n",
    "        inv_ytestpredict = concatenate((ytestpredtict, test_X[:, -(n_features-1):]), axis=1)\n",
    "        inv_ytestpredict = scaler.inverse_transform(inv_ytestpredict)\n",
    "        inv_ytestpredict = inv_ytestpredict[:,0]\n",
    "        # invert scaling for actual\n",
    "        test_y = test_y.reshape((len(test_y), 1))\n",
    "        inv_y = concatenate((test_y, test_X[:, -(n_features-1):]), axis=1)\n",
    "        inv_y = scaler.inverse_transform(inv_y)\n",
    "        inv_y = inv_y[:,0]\n",
    "        # calculate score\n",
    "        testMSE = mean_squared_error(inv_y, inv_ytestpredict)\n",
    "        \n",
    "        testRMSE = math.sqrt(testMSE)\n",
    "        \n",
    "        test_MAPE = mean_absolute_error(inv_y, inv_ytestpredict)/ (sum(inv_y)/len(inv_y)) * 100\n",
    "        \n",
    "        test_SMAPE = smape(inv_y, inv_ytestpredict)\n",
    "        \n",
    "        #rmse = math.sqrt(mean_squared_error(inv_y, inv_yhat))\n",
    "        #print('Test MSE: %.3f' % testMSE)\n",
    "        #print('Test RMSE: %.3f' % testRMSE)  \n",
    "        #print('Test MAPE: %.3f' % test_MAPE)\n",
    "        print('Optimizer: ' + optimizer_al[opt])    \n",
    "        print('Test SMAPE: %.3f' % test_SMAPE)   \n",
    "        if (test_SMAPE<min_sMAPE_lstm):\n",
    "            min_sMAPE_lstm=test_SMAPE\n",
    "            best_optimizer=opt\n",
    "print('The best optimizer function: ' + optimizer_al[best_optimizer]+ \", sMAPE: \" + str(min_sMAPE_lstm))   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Turning Epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch_ls=[50,60]\n",
    "#batch_size_ls=[10,20,50,100,200,500,1000]\n",
    "min_sMAPE_lstm = 100\n",
    "best_epoch=0\n",
    "\n",
    "k=0\n",
    "for ep in range(len(epoch_ls)):    \n",
    "        #Dataset for LSTM\n",
    "        dt=np.array([[j] for j in regionWisePickup_porto_data[0]])\n",
    "        #Take neighbour\n",
    "        #for h in range(k):\n",
    "        #    dt1=np.array([[j] for j in regionWisePickup_porto_data[k_neighbour[i][h+1]]])\n",
    "        #    dt=np.concatenate((dt,dt1),axis=1)\n",
    "        #Generate a time series with count of POI for stand\n",
    "        #standpoi=np.full((len(regionWisePickup_porto_data[i]), 1), poi[i])\n",
    "        #dt=np.concatenate((dt,standpoi),axis=1)\n",
    "        # ensure all data is float\n",
    "        values = dt.astype('float32')\n",
    "        # normalize features\n",
    "        scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "        scaled = scaler.fit_transform(values)\n",
    "        look_back = 5 #number of data look back - to predict, default 1\n",
    "        # frame as supervised learning\n",
    "        reframed = series_to_supervised(scaled, look_back, 1)\n",
    "        # split into train and test sets\n",
    "        values = reframed.values\n",
    "        train_size = int(len(values) * 0.7)\n",
    "        test_size = len(values) - train_size\n",
    "        #n_train_hours\n",
    "        train = values[:train_size, :]\n",
    "        test = values[train_size:, :]\n",
    "        # split into input and outputs\n",
    "        n_features = k + 1 # include itself and one extended feature\n",
    "        #n_features = k + 1\n",
    "        n_obs = look_back * n_features\n",
    "        train_X, train_y = train[:, :n_obs], train[:, -n_features]\n",
    "        test_X, test_y = test[:, :n_obs], test[:, -n_features]\n",
    "        #print(train_X.shape, len(train_X), train_y.shape)\n",
    "        # reshape input to be 3D [samples, timesteps, features]\n",
    "        train_X = train_X.reshape((train_X.shape[0], look_back, n_features))\n",
    "        test_X = test_X.reshape((test_X.shape[0], look_back, n_features))\n",
    "        #print(train_X.shape, train_y.shape, test_X.shape, test_y.shape)\n",
    "        # design network\n",
    "        model = Sequential()\n",
    "        model.add(LSTM(50, input_shape=(train_X.shape[1], train_X.shape[2])))\n",
    "        #model.add(LSTM(50, input_shape=(train_X.shape[1], train_X.shape[2]),return_sequences=True))\n",
    "        #model.add(LSTM(100,return_sequences=True))\n",
    "        #model.add(LSTM(50,return_sequences=True))\n",
    "        #model.add(LSTM(100))\n",
    "        model.add(Dense(1))\n",
    "        model.compile(loss='mae', optimizer='Adamax')\n",
    "        model.summary()\n",
    "        # fit network\n",
    "        history = model.fit(train_X, train_y, epochs=epoch_ls[ep], batch_size=50, validation_data=(test_X, test_y), verbose=2, shuffle=False)\n",
    "        # plot history\n",
    "        #plt.plot(history.history['loss'], label='train')\n",
    "        #plt.plot(history.history['val_loss'], label='test')\n",
    "        #plt.legend()\n",
    "        #plt.show()\n",
    "        #figname=\"Plot\"+str(i)+\".png\"\n",
    "        #plt.savefig(figname,formatt=\"png\")\n",
    "        \n",
    "        # make a prediction of train set\n",
    "        ytrainpredict = model.predict(train_X)\n",
    "        train_X = train_X.reshape((train_X.shape[0], look_back*n_features))\n",
    "        inv_ytrainpredict = concatenate((ytrainpredict, train_X[:, -(n_features-1):]), axis=1)\n",
    "        inv_ytrainpredict = scaler.inverse_transform(inv_ytrainpredict)\n",
    "        inv_ytrainpredict = inv_ytrainpredict[:,0]\n",
    "        train_y = train_y.reshape((len(train_y), 1))\n",
    "        inv_trainy = concatenate((train_y, train_X[:, -(n_features-1):]), axis=1)\n",
    "        inv_trainy = scaler.inverse_transform(inv_trainy)\n",
    "        inv_trainy = inv_trainy[:,0]\n",
    "        #calculate score\n",
    "        trainMSE = mean_squared_error(inv_trainy, inv_ytrainpredict)\n",
    "        \n",
    "        trainRMSE = math.sqrt(trainMSE)\n",
    "        \n",
    "        train_MAPE = mean_absolute_error(inv_trainy, inv_ytrainpredict)/ (sum(inv_trainy)/len(inv_trainy)) * 100\n",
    "        \n",
    "        train_SMAPE = smape(inv_trainy, inv_ytrainpredict)\n",
    "        \n",
    "        #print('Train MSE: %.3f' %trainMSE)\n",
    "        #print('Train RMSE: %.3f' %trainRMSE)\n",
    "        #print('Train MAPE: %.3f' %train_MAPE)\n",
    "        #print('Train SMAPE: %.3f' %train_SMAPE)\n",
    "        \n",
    "        \n",
    "        # make a prediction of test set\n",
    "        ytestpredtict = model.predict(test_X)        \n",
    "        test_X = test_X.reshape((test_X.shape[0], look_back*n_features))\n",
    "        # invert scaling for forecast\n",
    "        inv_ytestpredict = concatenate((ytestpredtict, test_X[:, -(n_features-1):]), axis=1)\n",
    "        inv_ytestpredict = scaler.inverse_transform(inv_ytestpredict)\n",
    "        inv_ytestpredict = inv_ytestpredict[:,0]\n",
    "        # invert scaling for actual\n",
    "        test_y = test_y.reshape((len(test_y), 1))\n",
    "        inv_y = concatenate((test_y, test_X[:, -(n_features-1):]), axis=1)\n",
    "        inv_y = scaler.inverse_transform(inv_y)\n",
    "        inv_y = inv_y[:,0]\n",
    "        # calculate score\n",
    "        testMSE = mean_squared_error(inv_y, inv_ytestpredict)\n",
    "        \n",
    "        testRMSE = math.sqrt(testMSE)\n",
    "        \n",
    "        test_MAPE = mean_absolute_error(inv_y, inv_ytestpredict)/ (sum(inv_y)/len(inv_y)) * 100\n",
    "        \n",
    "        test_SMAPE = smape(inv_y, inv_ytestpredict)\n",
    "        \n",
    "        #rmse = math.sqrt(mean_squared_error(inv_y, inv_yhat))\n",
    "        #print('Test MSE: %.3f' % testMSE)\n",
    "        #print('Test RMSE: %.3f' % testRMSE)  \n",
    "        #print('Test MAPE: %.3f' % test_MAPE)\n",
    "        print('Epoch: ' + str(epoch_ls[ep]))    \n",
    "        print('Test SMAPE: %.3f' % test_SMAPE)   \n",
    "        if (test_SMAPE<min_sMAPE_lstm):\n",
    "            min_sMAPE_lstm=test_SMAPE\n",
    "            best_epoch=ep\n",
    "print('The best epoch: ' + str(epoch_ls[best_epoch])+ \", sMAPE: \" + str(min_sMAPE_lstm)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "The best epoch: 40, sMAPE: 26.448763\n",
    "        Epoch: 30\n",
    "Test SMAPE: 26.667\n",
    "    Epoch: 25\n",
    "Test SMAPE: 26.521\n",
    "    Epoch: 15\n",
    "Test SMAPE: 26.526\n",
    "    Epoch: 20\n",
    "Test SMAPE: 26.540\n",
    "    Epoch: 10\n",
    "Test SMAPE: 26.561"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Batch size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#epoch_ls=[50,60]\n",
    "batch_size_ls=[10,20,50,100,200,500,1000]\n",
    "min_sMAPE_lstm = 100\n",
    "best_batch_size=0\n",
    "\n",
    "k=0\n",
    "for bs in range(len(batch_size_ls)):    \n",
    "        #Dataset for LSTM\n",
    "        dt=np.array([[j] for j in regionWisePickup_porto_data[0]])\n",
    "        #Take neighbour\n",
    "        #for h in range(k):\n",
    "        #    dt1=np.array([[j] for j in regionWisePickup_porto_data[k_neighbour[i][h+1]]])\n",
    "        #    dt=np.concatenate((dt,dt1),axis=1)\n",
    "        #Generate a time series with count of POI for stand\n",
    "        #standpoi=np.full((len(regionWisePickup_porto_data[i]), 1), poi[i])\n",
    "        #dt=np.concatenate((dt,standpoi),axis=1)\n",
    "        # ensure all data is float\n",
    "        values = dt.astype('float32')\n",
    "        # normalize features\n",
    "        scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "        scaled = scaler.fit_transform(values)\n",
    "        look_back = 5 #number of data look back - to predict, default 1\n",
    "        # frame as supervised learning\n",
    "        reframed = series_to_supervised(scaled, look_back, 1)\n",
    "        # split into train and test sets\n",
    "        values = reframed.values\n",
    "        train_size = int(len(values) * 0.7)\n",
    "        test_size = len(values) - train_size\n",
    "        #n_train_hours\n",
    "        train = values[:train_size, :]\n",
    "        test = values[train_size:, :]\n",
    "        # split into input and outputs\n",
    "        n_features = k + 1 # include itself and one extended feature\n",
    "        #n_features = k + 1\n",
    "        n_obs = look_back * n_features\n",
    "        train_X, train_y = train[:, :n_obs], train[:, -n_features]\n",
    "        test_X, test_y = test[:, :n_obs], test[:, -n_features]\n",
    "        #print(train_X.shape, len(train_X), train_y.shape)\n",
    "        # reshape input to be 3D [samples, timesteps, features]\n",
    "        train_X = train_X.reshape((train_X.shape[0], look_back, n_features))\n",
    "        test_X = test_X.reshape((test_X.shape[0], look_back, n_features))\n",
    "        #print(train_X.shape, train_y.shape, test_X.shape, test_y.shape)\n",
    "        # design network\n",
    "        model = Sequential()\n",
    "        model.add(LSTM(50, input_shape=(train_X.shape[1], train_X.shape[2])))\n",
    "        #model.add(LSTM(50, input_shape=(train_X.shape[1], train_X.shape[2]),return_sequences=True))\n",
    "        #model.add(LSTM(100,return_sequences=True))\n",
    "        #model.add(LSTM(50,return_sequences=True))\n",
    "        #model.add(LSTM(100))\n",
    "        model.add(Dense(1))\n",
    "        model.compile(loss='mae', optimizer='Adamax')\n",
    "        model.summary()\n",
    "        # fit network\n",
    "        history = model.fit(train_X, train_y, epochs=25, batch_size=batch_size_ls[bs], validation_data=(test_X, test_y), verbose=2, shuffle=False)\n",
    "        # plot history\n",
    "        #plt.plot(history.history['loss'], label='train')\n",
    "        #plt.plot(history.history['val_loss'], label='test')\n",
    "        #plt.legend()\n",
    "        #plt.show()\n",
    "        #figname=\"Plot\"+str(i)+\".png\"\n",
    "        #plt.savefig(figname,formatt=\"png\")\n",
    "        \n",
    "        # make a prediction of train set\n",
    "        ytrainpredict = model.predict(train_X)\n",
    "        train_X = train_X.reshape((train_X.shape[0], look_back*n_features))\n",
    "        inv_ytrainpredict = concatenate((ytrainpredict, train_X[:, -(n_features-1):]), axis=1)\n",
    "        inv_ytrainpredict = scaler.inverse_transform(inv_ytrainpredict)\n",
    "        inv_ytrainpredict = inv_ytrainpredict[:,0]\n",
    "        train_y = train_y.reshape((len(train_y), 1))\n",
    "        inv_trainy = concatenate((train_y, train_X[:, -(n_features-1):]), axis=1)\n",
    "        inv_trainy = scaler.inverse_transform(inv_trainy)\n",
    "        inv_trainy = inv_trainy[:,0]\n",
    "        #calculate score\n",
    "        trainMSE = mean_squared_error(inv_trainy, inv_ytrainpredict)\n",
    "        \n",
    "        trainRMSE = math.sqrt(trainMSE)\n",
    "        \n",
    "        train_MAPE = mean_absolute_error(inv_trainy, inv_ytrainpredict)/ (sum(inv_trainy)/len(inv_trainy)) * 100\n",
    "        \n",
    "        train_SMAPE = smape(inv_trainy, inv_ytrainpredict)\n",
    "        \n",
    "        #print('Train MSE: %.3f' %trainMSE)\n",
    "        #print('Train RMSE: %.3f' %trainRMSE)\n",
    "        #print('Train MAPE: %.3f' %train_MAPE)\n",
    "        #print('Train SMAPE: %.3f' %train_SMAPE)\n",
    "        \n",
    "        \n",
    "        # make a prediction of test set\n",
    "        ytestpredtict = model.predict(test_X)        \n",
    "        test_X = test_X.reshape((test_X.shape[0], look_back*n_features))\n",
    "        # invert scaling for forecast\n",
    "        inv_ytestpredict = concatenate((ytestpredtict, test_X[:, -(n_features-1):]), axis=1)\n",
    "        inv_ytestpredict = scaler.inverse_transform(inv_ytestpredict)\n",
    "        inv_ytestpredict = inv_ytestpredict[:,0]\n",
    "        # invert scaling for actual\n",
    "        test_y = test_y.reshape((len(test_y), 1))\n",
    "        inv_y = concatenate((test_y, test_X[:, -(n_features-1):]), axis=1)\n",
    "        inv_y = scaler.inverse_transform(inv_y)\n",
    "        inv_y = inv_y[:,0]\n",
    "        # calculate score\n",
    "        testMSE = mean_squared_error(inv_y, inv_ytestpredict)\n",
    "        \n",
    "        testRMSE = math.sqrt(testMSE)\n",
    "        \n",
    "        test_MAPE = mean_absolute_error(inv_y, inv_ytestpredict)/ (sum(inv_y)/len(inv_y)) * 100\n",
    "        \n",
    "        test_SMAPE = smape(inv_y, inv_ytestpredict)\n",
    "        \n",
    "        #rmse = math.sqrt(mean_squared_error(inv_y, inv_yhat))\n",
    "        #print('Test MSE: %.3f' % testMSE)\n",
    "        #print('Test RMSE: %.3f' % testRMSE)  \n",
    "        #print('Test MAPE: %.3f' % test_MAPE)\n",
    "        print('Batch size: ' + str(batch_size_ls[bs]))    \n",
    "        print('Test SMAPE: %.3f' % test_SMAPE)   \n",
    "        if (test_SMAPE<min_sMAPE_lstm):\n",
    "            min_sMAPE_lstm=test_SMAPE\n",
    "            best_batch_size=bs\n",
    "print('The best batch size: ' + str(batch_size_ls[best_batch_size])+ \", sMAPE: \" + str(min_sMAPE_lstm)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dropout_ls=[0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]\n",
    "min_sMAPE_lstm = 100\n",
    "best_dropout=0.0\n",
    "\n",
    "k=0\n",
    "for dr in range(len(dropout_ls)):    \n",
    "        #Dataset for LSTM\n",
    "        dt=np.array([[j] for j in regionWisePickup_porto_data[0]])\n",
    "        #Take neighbour\n",
    "        #for h in range(k):\n",
    "        #    dt1=np.array([[j] for j in regionWisePickup_porto_data[k_neighbour[i][h+1]]])\n",
    "        #    dt=np.concatenate((dt,dt1),axis=1)\n",
    "        #Generate a time series with count of POI for stand\n",
    "        #standpoi=np.full((len(regionWisePickup_porto_data[i]), 1), poi[i])\n",
    "        #dt=np.concatenate((dt,standpoi),axis=1)\n",
    "        # ensure all data is float\n",
    "        values = dt.astype('float32')\n",
    "        # normalize features\n",
    "        scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "        scaled = scaler.fit_transform(values)\n",
    "        look_back = 5 #number of data look back - to predict, default 1\n",
    "        # frame as supervised learning\n",
    "        reframed = series_to_supervised(scaled, look_back, 1)\n",
    "        # split into train and test sets\n",
    "        values = reframed.values\n",
    "        train_size = int(len(values) * 0.7)\n",
    "        test_size = len(values) - train_size\n",
    "        #n_train_hours\n",
    "        train = values[:train_size, :]\n",
    "        test = values[train_size:, :]\n",
    "        # split into input and outputs\n",
    "        n_features = k + 1 # include itself and one extended feature\n",
    "        #n_features = k + 1\n",
    "        n_obs = look_back * n_features\n",
    "        train_X, train_y = train[:, :n_obs], train[:, -n_features]\n",
    "        test_X, test_y = test[:, :n_obs], test[:, -n_features]\n",
    "        #print(train_X.shape, len(train_X), train_y.shape)\n",
    "        # reshape input to be 3D [samples, timesteps, features]\n",
    "        train_X = train_X.reshape((train_X.shape[0], look_back, n_features))\n",
    "        test_X = test_X.reshape((test_X.shape[0], look_back, n_features))\n",
    "        #print(train_X.shape, train_y.shape, test_X.shape, test_y.shape)\n",
    "        # design network\n",
    "        model = Sequential()\n",
    "        model.add(LSTM(50, input_shape=(train_X.shape[1], train_X.shape[2])))\n",
    "        #model.add(LSTM(50, input_shape=(train_X.shape[1], train_X.shape[2]),return_sequences=True))\n",
    "        #model.add(LSTM(100,return_sequences=True))\n",
    "        #model.add(LSTM(50,return_sequences=True))\n",
    "        #model.add(LSTM(100))\n",
    "        model.add(Dense(1))\n",
    "        model.add(Dropout(dropout_ls[dr]))\n",
    "        model.compile(loss='mae', optimizer='Adamax')\n",
    "        model.summary()\n",
    "        # fit network\n",
    "        history = model.fit(train_X, train_y, epochs=25, batch_size=100, validation_data=(test_X, test_y), verbose=2, shuffle=False)\n",
    "        # plot history\n",
    "        #plt.plot(history.history['loss'], label='train')\n",
    "        #plt.plot(history.history['val_loss'], label='test')\n",
    "        #plt.legend()\n",
    "        #plt.show()\n",
    "        #figname=\"Plot\"+str(i)+\".png\"\n",
    "        #plt.savefig(figname,formatt=\"png\")\n",
    "        \n",
    "        # make a prediction of train set\n",
    "        ytrainpredict = model.predict(train_X)\n",
    "        train_X = train_X.reshape((train_X.shape[0], look_back*n_features))\n",
    "        inv_ytrainpredict = concatenate((ytrainpredict, train_X[:, -(n_features-1):]), axis=1)\n",
    "        inv_ytrainpredict = scaler.inverse_transform(inv_ytrainpredict)\n",
    "        inv_ytrainpredict = inv_ytrainpredict[:,0]\n",
    "        train_y = train_y.reshape((len(train_y), 1))\n",
    "        inv_trainy = concatenate((train_y, train_X[:, -(n_features-1):]), axis=1)\n",
    "        inv_trainy = scaler.inverse_transform(inv_trainy)\n",
    "        inv_trainy = inv_trainy[:,0]\n",
    "        #calculate score\n",
    "        trainMSE = mean_squared_error(inv_trainy, inv_ytrainpredict)\n",
    "        \n",
    "        trainRMSE = math.sqrt(trainMSE)\n",
    "        \n",
    "        train_MAPE = mean_absolute_error(inv_trainy, inv_ytrainpredict)/ (sum(inv_trainy)/len(inv_trainy)) * 100\n",
    "        \n",
    "        train_SMAPE = smape(inv_trainy, inv_ytrainpredict)\n",
    "        \n",
    "        #print('Train MSE: %.3f' %trainMSE)\n",
    "        #print('Train RMSE: %.3f' %trainRMSE)\n",
    "        #print('Train MAPE: %.3f' %train_MAPE)\n",
    "        #print('Train SMAPE: %.3f' %train_SMAPE)\n",
    "        \n",
    "        \n",
    "        # make a prediction of test set\n",
    "        ytestpredtict = model.predict(test_X)        \n",
    "        test_X = test_X.reshape((test_X.shape[0], look_back*n_features))\n",
    "        # invert scaling for forecast\n",
    "        inv_ytestpredict = concatenate((ytestpredtict, test_X[:, -(n_features-1):]), axis=1)\n",
    "        inv_ytestpredict = scaler.inverse_transform(inv_ytestpredict)\n",
    "        inv_ytestpredict = inv_ytestpredict[:,0]\n",
    "        # invert scaling for actual\n",
    "        test_y = test_y.reshape((len(test_y), 1))\n",
    "        inv_y = concatenate((test_y, test_X[:, -(n_features-1):]), axis=1)\n",
    "        inv_y = scaler.inverse_transform(inv_y)\n",
    "        inv_y = inv_y[:,0]\n",
    "        # calculate score\n",
    "        testMSE = mean_squared_error(inv_y, inv_ytestpredict)\n",
    "        \n",
    "        testRMSE = math.sqrt(testMSE)\n",
    "        \n",
    "        test_MAPE = mean_absolute_error(inv_y, inv_ytestpredict)/ (sum(inv_y)/len(inv_y)) * 100\n",
    "        \n",
    "        test_SMAPE = smape(inv_y, inv_ytestpredict)\n",
    "        \n",
    "        #rmse = math.sqrt(mean_squared_error(inv_y, inv_yhat))\n",
    "        #print('Test MSE: %.3f' % testMSE)\n",
    "        #print('Test RMSE: %.3f' % testRMSE)  \n",
    "        #print('Test MAPE: %.3f' % test_MAPE)\n",
    "        print('Drop out: ' + str(dropout_ls[dr]))    \n",
    "        print('Test SMAPE: %.3f' % test_SMAPE)   \n",
    "        if (test_SMAPE<min_sMAPE_lstm):\n",
    "            min_sMAPE_lstm=test_SMAPE\n",
    "            best_dropout=dr\n",
    "print('The best dropout: ' + str(dropout_ls[best_dropout])+ \", sMAPE: \" + str(min_sMAPE_lstm)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Number of neuron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neuron_ls=[10,20,30,40,50,60,70,80,90,100,150,200]\n",
    "min_sMAPE_lstm = 100\n",
    "best_neuron=10\n",
    "\n",
    "k=0\n",
    "for nr in range(len(neuron_ls)):    \n",
    "        #Dataset for LSTM\n",
    "        dt=np.array([[j] for j in regionWisePickup_porto_data[0]])\n",
    "        #Take neighbour\n",
    "        #for h in range(k):\n",
    "        #    dt1=np.array([[j] for j in regionWisePickup_porto_data[k_neighbour[i][h+1]]])\n",
    "        #    dt=np.concatenate((dt,dt1),axis=1)\n",
    "        #Generate a time series with count of POI for stand\n",
    "        #standpoi=np.full((len(regionWisePickup_porto_data[i]), 1), poi[i])\n",
    "        #dt=np.concatenate((dt,standpoi),axis=1)\n",
    "        # ensure all data is float\n",
    "        values = dt.astype('float32')\n",
    "        # normalize features\n",
    "        scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "        scaled = scaler.fit_transform(values)\n",
    "        look_back = 5 #number of data look back - to predict, default 1\n",
    "        # frame as supervised learning\n",
    "        reframed = series_to_supervised(scaled, look_back, 1)\n",
    "        # split into train and test sets\n",
    "        values = reframed.values\n",
    "        train_size = int(len(values) * 0.7)\n",
    "        test_size = len(values) - train_size\n",
    "        #n_train_hours\n",
    "        train = values[:train_size, :]\n",
    "        test = values[train_size:, :]\n",
    "        # split into input and outputs\n",
    "        n_features = k + 1 # include itself and one extended feature\n",
    "        #n_features = k + 1\n",
    "        n_obs = look_back * n_features\n",
    "        train_X, train_y = train[:, :n_obs], train[:, -n_features]\n",
    "        test_X, test_y = test[:, :n_obs], test[:, -n_features]\n",
    "        #print(train_X.shape, len(train_X), train_y.shape)\n",
    "        # reshape input to be 3D [samples, timesteps, features]\n",
    "        train_X = train_X.reshape((train_X.shape[0], look_back, n_features))\n",
    "        test_X = test_X.reshape((test_X.shape[0], look_back, n_features))\n",
    "        #print(train_X.shape, train_y.shape, test_X.shape, test_y.shape)\n",
    "        # design network\n",
    "        model = Sequential()\n",
    "        model.add(LSTM(neuron_ls[nr], input_shape=(train_X.shape[1], train_X.shape[2])))\n",
    "        #model.add(LSTM(50, input_shape=(train_X.shape[1], train_X.shape[2]),return_sequences=True))\n",
    "        #model.add(LSTM(100,return_sequences=True))\n",
    "        #model.add(LSTM(50,return_sequences=True))\n",
    "        #model.add(LSTM(100))\n",
    "        model.add(Dense(1))\n",
    "        model.add(Dropout(0.7))\n",
    "        model.compile(loss='mae', optimizer='Adamax')\n",
    "        model.summary()\n",
    "        # fit network\n",
    "        history = model.fit(train_X, train_y, epochs=25, batch_size=100, validation_data=(test_X, test_y), verbose=2, shuffle=False)\n",
    "        # plot history\n",
    "        #plt.plot(history.history['loss'], label='train')\n",
    "        #plt.plot(history.history['val_loss'], label='test')\n",
    "        #plt.legend()\n",
    "        #plt.show()\n",
    "        #figname=\"Plot\"+str(i)+\".png\"\n",
    "        #plt.savefig(figname,formatt=\"png\")\n",
    "        \n",
    "        # make a prediction of train set\n",
    "        ytrainpredict = model.predict(train_X)\n",
    "        train_X = train_X.reshape((train_X.shape[0], look_back*n_features))\n",
    "        inv_ytrainpredict = concatenate((ytrainpredict, train_X[:, -(n_features-1):]), axis=1)\n",
    "        inv_ytrainpredict = scaler.inverse_transform(inv_ytrainpredict)\n",
    "        inv_ytrainpredict = inv_ytrainpredict[:,0]\n",
    "        train_y = train_y.reshape((len(train_y), 1))\n",
    "        inv_trainy = concatenate((train_y, train_X[:, -(n_features-1):]), axis=1)\n",
    "        inv_trainy = scaler.inverse_transform(inv_trainy)\n",
    "        inv_trainy = inv_trainy[:,0]\n",
    "        #calculate score\n",
    "        trainMSE = mean_squared_error(inv_trainy, inv_ytrainpredict)\n",
    "        \n",
    "        trainRMSE = math.sqrt(trainMSE)\n",
    "        \n",
    "        train_MAPE = mean_absolute_error(inv_trainy, inv_ytrainpredict)/ (sum(inv_trainy)/len(inv_trainy)) * 100\n",
    "        \n",
    "        train_SMAPE = smape(inv_trainy, inv_ytrainpredict)\n",
    "        \n",
    "        #print('Train MSE: %.3f' %trainMSE)\n",
    "        #print('Train RMSE: %.3f' %trainRMSE)\n",
    "        #print('Train MAPE: %.3f' %train_MAPE)\n",
    "        #print('Train SMAPE: %.3f' %train_SMAPE)\n",
    "        \n",
    "        \n",
    "        # make a prediction of test set\n",
    "        ytestpredtict = model.predict(test_X)        \n",
    "        test_X = test_X.reshape((test_X.shape[0], look_back*n_features))\n",
    "        # invert scaling for forecast\n",
    "        inv_ytestpredict = concatenate((ytestpredtict, test_X[:, -(n_features-1):]), axis=1)\n",
    "        inv_ytestpredict = scaler.inverse_transform(inv_ytestpredict)\n",
    "        inv_ytestpredict = inv_ytestpredict[:,0]\n",
    "        # invert scaling for actual\n",
    "        test_y = test_y.reshape((len(test_y), 1))\n",
    "        inv_y = concatenate((test_y, test_X[:, -(n_features-1):]), axis=1)\n",
    "        inv_y = scaler.inverse_transform(inv_y)\n",
    "        inv_y = inv_y[:,0]\n",
    "        # calculate score\n",
    "        testMSE = mean_squared_error(inv_y, inv_ytestpredict)\n",
    "        \n",
    "        testRMSE = math.sqrt(testMSE)\n",
    "        \n",
    "        test_MAPE = mean_absolute_error(inv_y, inv_ytestpredict)/ (sum(inv_y)/len(inv_y)) * 100\n",
    "        \n",
    "        test_SMAPE = smape(inv_y, inv_ytestpredict)\n",
    "        \n",
    "        #rmse = math.sqrt(mean_squared_error(inv_y, inv_yhat))\n",
    "        #print('Test MSE: %.3f' % testMSE)\n",
    "        #print('Test RMSE: %.3f' % testRMSE)  \n",
    "        #print('Test MAPE: %.3f' % test_MAPE)\n",
    "        print('Neuron: ' + str(neuron_ls[nr]))    \n",
    "        print('Test SMAPE: %.3f' % test_SMAPE)   \n",
    "        if (test_SMAPE<min_sMAPE_lstm):\n",
    "            min_sMAPE_lstm=test_SMAPE\n",
    "            best_neuron=nr\n",
    "print('The best number of neuron: ' + str(neuron_ls[best_neuron])+ \", sMAPE: \" + str(min_sMAPE_lstm)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Number of layer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_layer=4\n",
    "min_sMAPE_lstm = 100\n",
    "best_layer=1\n",
    "\n",
    "k=0\n",
    "for nl in range(0,n_layer):    \n",
    "        #Dataset for LSTM\n",
    "        dt=np.array([[j] for j in regionWisePickup_porto_data[0]])\n",
    "        #Take neighbour\n",
    "        #for h in range(k):\n",
    "        #    dt1=np.array([[j] for j in regionWisePickup_porto_data[k_neighbour[i][h+1]]])\n",
    "        #    dt=np.concatenate((dt,dt1),axis=1)\n",
    "        #Generate a time series with count of POI for stand\n",
    "        #standpoi=np.full((len(regionWisePickup_porto_data[i]), 1), poi[i])\n",
    "        #dt=np.concatenate((dt,standpoi),axis=1)\n",
    "        # ensure all data is float\n",
    "        values = dt.astype('float32')\n",
    "        # normalize features\n",
    "        scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "        scaled = scaler.fit_transform(values)\n",
    "        look_back = 5 #number of data look back - to predict, default 1\n",
    "        # frame as supervised learning\n",
    "        reframed = series_to_supervised(scaled, look_back, 1)\n",
    "        # split into train and test sets\n",
    "        values = reframed.values\n",
    "        train_size = int(len(values) * 0.7)\n",
    "        test_size = len(values) - train_size\n",
    "        #n_train_hours\n",
    "        train = values[:train_size, :]\n",
    "        test = values[train_size:, :]\n",
    "        # split into input and outputs\n",
    "        n_features = k + 1 # include itself and one extended feature\n",
    "        #n_features = k + 1\n",
    "        n_obs = look_back * n_features\n",
    "        train_X, train_y = train[:, :n_obs], train[:, -n_features]\n",
    "        test_X, test_y = test[:, :n_obs], test[:, -n_features]\n",
    "        #print(train_X.shape, len(train_X), train_y.shape)\n",
    "        # reshape input to be 3D [samples, timesteps, features]\n",
    "        train_X = train_X.reshape((train_X.shape[0], look_back, n_features))\n",
    "        test_X = test_X.reshape((test_X.shape[0], look_back, n_features))\n",
    "        #print(train_X.shape, train_y.shape, test_X.shape, test_y.shape)\n",
    "        # design network\n",
    "        model = Sequential()\n",
    "        if (nl==0):\n",
    "            model.add(LSTM(200, input_shape=(train_X.shape[1], train_X.shape[2])))\n",
    "            \n",
    "        if (nl==1):\n",
    "            model.add(LSTM(200, input_shape=(train_X.shape[1], train_X.shape[2]),return_sequences=True))\n",
    "            model.add(LSTM(200))\n",
    "        if (nl==2):\n",
    "            model.add(LSTM(200, input_shape=(train_X.shape[1], train_X.shape[2]),return_sequences=True))\n",
    "            model.add(LSTM(200,return_sequences=True))\n",
    "            model.add(LSTM(200))\n",
    "        if (nl==3):\n",
    "            model.add(LSTM(200, input_shape=(train_X.shape[1], train_X.shape[2]),return_sequences=True))\n",
    "            model.add(LSTM(200,return_sequences=True))\n",
    "            model.add(LSTM(200,return_sequences=True))\n",
    "            model.add(LSTM(200))\n",
    "        model.add(Dense(1))\n",
    "        model.add(Dropout(0.7))\n",
    "        model.compile(loss='mae', optimizer='Adamax')\n",
    "        model.summary()\n",
    "        # fit network\n",
    "        history = model.fit(train_X, train_y, epochs=25, batch_size=100, validation_data=(test_X, test_y), verbose=2, shuffle=False)\n",
    "        # plot history\n",
    "        #plt.plot(history.history['loss'], label='train')\n",
    "        #plt.plot(history.history['val_loss'], label='test')\n",
    "        #plt.legend()\n",
    "        #plt.show()\n",
    "        #figname=\"Plot\"+str(i)+\".png\"\n",
    "        #plt.savefig(figname,formatt=\"png\")\n",
    "        \n",
    "        # make a prediction of train set\n",
    "        ytrainpredict = model.predict(train_X)\n",
    "        train_X = train_X.reshape((train_X.shape[0], look_back*n_features))\n",
    "        inv_ytrainpredict = concatenate((ytrainpredict, train_X[:, -(n_features-1):]), axis=1)\n",
    "        inv_ytrainpredict = scaler.inverse_transform(inv_ytrainpredict)\n",
    "        inv_ytrainpredict = inv_ytrainpredict[:,0]\n",
    "        train_y = train_y.reshape((len(train_y), 1))\n",
    "        inv_trainy = concatenate((train_y, train_X[:, -(n_features-1):]), axis=1)\n",
    "        inv_trainy = scaler.inverse_transform(inv_trainy)\n",
    "        inv_trainy = inv_trainy[:,0]\n",
    "        #calculate score\n",
    "        trainMSE = mean_squared_error(inv_trainy, inv_ytrainpredict)\n",
    "        \n",
    "        trainRMSE = math.sqrt(trainMSE)\n",
    "        \n",
    "        train_MAPE = mean_absolute_error(inv_trainy, inv_ytrainpredict)/ (sum(inv_trainy)/len(inv_trainy)) * 100\n",
    "        \n",
    "        train_SMAPE = smape(inv_trainy, inv_ytrainpredict)\n",
    "        \n",
    "        #print('Train MSE: %.3f' %trainMSE)\n",
    "        #print('Train RMSE: %.3f' %trainRMSE)\n",
    "        #print('Train MAPE: %.3f' %train_MAPE)\n",
    "        #print('Train SMAPE: %.3f' %train_SMAPE)\n",
    "        \n",
    "        \n",
    "        # make a prediction of test set\n",
    "        ytestpredtict = model.predict(test_X)        \n",
    "        test_X = test_X.reshape((test_X.shape[0], look_back*n_features))\n",
    "        # invert scaling for forecast\n",
    "        inv_ytestpredict = concatenate((ytestpredtict, test_X[:, -(n_features-1):]), axis=1)\n",
    "        inv_ytestpredict = scaler.inverse_transform(inv_ytestpredict)\n",
    "        inv_ytestpredict = inv_ytestpredict[:,0]\n",
    "        # invert scaling for actual\n",
    "        test_y = test_y.reshape((len(test_y), 1))\n",
    "        inv_y = concatenate((test_y, test_X[:, -(n_features-1):]), axis=1)\n",
    "        inv_y = scaler.inverse_transform(inv_y)\n",
    "        inv_y = inv_y[:,0]\n",
    "        # calculate score\n",
    "        testMSE = mean_squared_error(inv_y, inv_ytestpredict)\n",
    "        \n",
    "        testRMSE = math.sqrt(testMSE)\n",
    "        \n",
    "        test_MAPE = mean_absolute_error(inv_y, inv_ytestpredict)/ (sum(inv_y)/len(inv_y)) * 100\n",
    "        \n",
    "        test_SMAPE = smape(inv_y, inv_ytestpredict)\n",
    "        \n",
    "        #rmse = math.sqrt(mean_squared_error(inv_y, inv_yhat))\n",
    "        #print('Test MSE: %.3f' % testMSE)\n",
    "        #print('Test RMSE: %.3f' % testRMSE)  \n",
    "        #print('Test MAPE: %.3f' % test_MAPE)\n",
    "        print('Layer: ' + str(nl))    \n",
    "        print('Test SMAPE: %.3f' % test_SMAPE)   \n",
    "        if (test_SMAPE<min_sMAPE_lstm):\n",
    "            min_sMAPE_lstm=test_SMAPE\n",
    "            best_layer=nl\n",
    "print('The best number of layer: ' + str(best_layer)+ \", sMAPE: \" + str(min_sMAPE_lstm)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Turning k-NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kNN=[1,3,5,7,9,11,13,15]\n",
    "min_sMAPE_lstm = 100\n",
    "best_k=1\n",
    "numberOfClusters = 1\n",
    "#k = 0 #number of neighbours\n",
    "for k in kNN:    \n",
    "        #print(\"Stand: %d\" %i)\n",
    "    #if (i in count_points_new):        \n",
    "        #print(\"Number of points: %d\" %count_points[i])\n",
    "        #Dataset for LSTM\n",
    "        dt=np.array([[j] for j in regionWisePickup_porto_data[0]])\n",
    "        #Take neighbour\n",
    "        for h in range(k):\n",
    "            dt1=np.array([[j] for j in regionWisePickup_porto_data[k_neighbour[i][h+1]]])\n",
    "            dt=np.concatenate((dt,dt1),axis=1)\n",
    "        #Generate a time series with count of POI for stand\n",
    "        #standpoi=np.full((len(regionWisePickup_porto_data[i]), 1), poi[i])\n",
    "        #dt=np.concatenate((dt,standpoi),axis=1)\n",
    "        # ensure all data is float\n",
    "        values = dt.astype('float32')\n",
    "        # normalize features\n",
    "        scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "        scaled = scaler.fit_transform(values)\n",
    "        look_back = 5 #number of data look back - to predict, default 1\n",
    "        # frame as supervised learning\n",
    "        reframed = series_to_supervised(scaled, look_back, 1)\n",
    "        # split into train and test sets\n",
    "        values = reframed.values\n",
    "        train_size = int(len(values) * 0.7)\n",
    "        test_size = len(values) - train_size\n",
    "        #n_train_hours\n",
    "        train = values[:train_size, :]\n",
    "        test = values[train_size:, :]\n",
    "        # split into input and outputs\n",
    "        #n_features = k + 2 # include itself and one extended feature\n",
    "        n_features = k + 1\n",
    "        n_obs = look_back * n_features\n",
    "        train_X, train_y = train[:, :n_obs], train[:, -n_features]\n",
    "        test_X, test_y = test[:, :n_obs], test[:, -n_features]\n",
    "        #print(train_X.shape, len(train_X), train_y.shape)\n",
    "        # reshape input to be 3D [samples, timesteps, features]\n",
    "        train_X = train_X.reshape((train_X.shape[0], look_back, n_features))\n",
    "        test_X = test_X.reshape((test_X.shape[0], look_back, n_features))\n",
    "        #print(train_X.shape, train_y.shape, test_X.shape, test_y.shape)\n",
    "        # design network\n",
    "        model = Sequential()\n",
    "        model.add(LSTM(200, input_shape=(train_X.shape[1], train_X.shape[2]),return_sequences=True))\n",
    "        model.add(LSTM(200))\n",
    "        model.add(Dense(1))\n",
    "        model.compile(loss='mae', optimizer='Adamax')\n",
    "        model.summary()\n",
    "        # fit network\n",
    "        history = model.fit(train_X, train_y, epochs=25, batch_size=100, validation_data=(test_X, test_y), verbose=2, shuffle=False)\n",
    "        # plot history\n",
    "        plt.plot(history.history['loss'], label='train')\n",
    "        plt.plot(history.history['val_loss'], label='test')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "        figname=\"Plot_LSTM_Turning_KNN_\"+str(k)+\".png\"\n",
    "        plt.savefig(figname,formatt=\"png\")\n",
    "        \n",
    "        # make a prediction of train set\n",
    "        ytrainpredict = model.predict(train_X)\n",
    "        train_X = train_X.reshape((train_X.shape[0], look_back*n_features))\n",
    "        inv_ytrainpredict = concatenate((ytrainpredict, train_X[:, -(n_features-1):]), axis=1)\n",
    "        inv_ytrainpredict = scaler.inverse_transform(inv_ytrainpredict)\n",
    "        inv_ytrainpredict = inv_ytrainpredict[:,0]\n",
    "        train_y = train_y.reshape((len(train_y), 1))\n",
    "        inv_trainy = concatenate((train_y, train_X[:, -(n_features-1):]), axis=1)\n",
    "        inv_trainy = scaler.inverse_transform(inv_trainy)\n",
    "        inv_trainy = inv_trainy[:,0]\n",
    "        #calculate score\n",
    "        trainMSE = mean_squared_error(inv_trainy, inv_ytrainpredict)\n",
    "        #btrainMSE.append(trainMSE)\n",
    "        trainRMSE = math.sqrt(trainMSE)\n",
    "        #btrainRMSE.append(trainRMSE)\n",
    "        train_MAPE = mean_absolute_error(inv_trainy, inv_ytrainpredict)/ (sum(inv_trainy)/len(inv_trainy)) * 100\n",
    "        #btrainMAPE.append(train_MAPE)\n",
    "        train_SMAPE = smape(inv_trainy, inv_ytrainpredict)\n",
    "        #btrainSMAPE.append(train_SMAPE)        \n",
    "        #print('Train MSE: %.3f' %trainMSE)\n",
    "        #print('Train RMSE: %.3f' %trainRMSE)\n",
    "        #print('Train MAPE: %.3f' %train_MAPE)\n",
    "        #print('Train SMAPE: %.3f' %train_SMAPE)\n",
    "        \n",
    "        \n",
    "        # make a prediction of test set\n",
    "        ytestpredtict = model.predict(test_X)        \n",
    "        test_X = test_X.reshape((test_X.shape[0], look_back*n_features))\n",
    "        # invert scaling for forecast\n",
    "        inv_ytestpredict = concatenate((ytestpredtict, test_X[:, -(n_features-1):]), axis=1)\n",
    "        inv_ytestpredict = scaler.inverse_transform(inv_ytestpredict)\n",
    "        inv_ytestpredict = inv_ytestpredict[:,0]\n",
    "        # invert scaling for actual\n",
    "        test_y = test_y.reshape((len(test_y), 1))\n",
    "        inv_y = concatenate((test_y, test_X[:, -(n_features-1):]), axis=1)\n",
    "        inv_y = scaler.inverse_transform(inv_y)\n",
    "        inv_y = inv_y[:,0]\n",
    "        # calculate score\n",
    "        testMSE = mean_squared_error(inv_y, inv_ytestpredict)\n",
    "        #btestMSE.append(testMSE)\n",
    "        testRMSE = math.sqrt(testMSE)\n",
    "        #btestRMSE.append(testRMSE)\n",
    "        test_MAPE = mean_absolute_error(inv_y, inv_ytestpredict)/ (sum(inv_y)/len(inv_y)) * 100\n",
    "        #btestMAPE.append(test_MAPE)\n",
    "        test_SMAPE = smape(inv_y, inv_ytestpredict)\n",
    "        #btestSMAPE.append(test_SMAPE)    \n",
    "        #rmse = math.sqrt(mean_squared_error(inv_y, inv_yhat))\n",
    "        #print('Test MSE: %.3f' % testMSE)\n",
    "        #print('Test RMSE: %.3f' % testRMSE)  \n",
    "        #print('Test MAPE: %.3f' % test_MAPE)\n",
    "        print('k = ' + str(k))\n",
    "        print('Test SMAPE: %.3f' % test_SMAPE) \n",
    "        if (test_SMAPE<min_sMAPE_lstm):\n",
    "            min_sMAPE_lstm=test_SMAPE\n",
    "            best_k=k\n",
    "print('The best number of kNN: ' + str(best_k)+ \", sMAPE: \" + str(min_sMAPE_lstm)) \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "btrainMSE=[]\n",
    "btestMSE=[]\n",
    "btrainRMSE=[]\n",
    "btestRMSE=[]\n",
    "btrainMAPE=[]\n",
    "btestMAPE=[]\n",
    "btrainSMAPE=[]\n",
    "btestSMAPE=[]\n",
    "\n",
    "numberOfClusters = 63\n",
    "k = 0 #number of neighbours\n",
    "for i in range(numberOfClusters):    \n",
    "        print(\"Stand: %d\" %i)\n",
    "    #if (i in count_points_new):        \n",
    "        #print(\"Number of points: %d\" %count_points[i])\n",
    "        #Dataset for LSTM\n",
    "        #dt=np.array([[j] for j in regionWisePickup_porto_data[i]])\n",
    "                #Dataset for LSTM\n",
    "        dt=np.array([[j] for j in regionWisePickup_porto_data[i]])\n",
    "\n",
    "        #Generate a time series with count of POI for stand\n",
    "        standpoi=np.array([[j] for j in standvisit[i]])\n",
    "        \n",
    "        dt=np.concatenate((dt,standpoi),axis=1)\n",
    "        #Take neighbour\n",
    "        #for h in range(k):\n",
    "        #    dt1=np.array([[j] for j in regionWisePickup_porto_data[k_neighbour[i][h+1]]])\n",
    "        #    dt=np.concatenate((dt,dt1),axis=1)\n",
    "        #Generate a time series with count of POI for stand\n",
    "        #standpoi=np.full((len(regionWisePickup_porto_data[i]), 1), poi[i])\n",
    "        #dt=np.concatenate((dt,standpoi),axis=1)\n",
    "        # ensure all data is float\n",
    "        values = dt.astype('float32')\n",
    "        # normalize features\n",
    "        scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "        scaled = scaler.fit_transform(values)\n",
    "        look_back = 5 #number of data look back - to predict, default 1\n",
    "        # frame as supervised learning\n",
    "        reframed = series_to_supervised(scaled, look_back, 1)\n",
    "        # split into train and test sets\n",
    "        values = reframed.values\n",
    "        train_size = int(len(values) * 0.7)\n",
    "        test_size = len(values) - train_size\n",
    "        #n_train_hours\n",
    "        train = values[:train_size, :]\n",
    "        test = values[train_size:, :]\n",
    "        # split into input and outputs\n",
    "        #n_features = k + 2 # include itself and one extended feature\n",
    "        n_features = k + 1\n",
    "        n_obs = look_back * n_features\n",
    "        train_X, train_y = train[:, :n_obs], train[:, -n_features]\n",
    "        test_X, test_y = test[:, :n_obs], test[:, -n_features]\n",
    "        #print(train_X.shape, len(train_X), train_y.shape)\n",
    "        # reshape input to be 3D [samples, timesteps, features]\n",
    "        train_X = train_X.reshape((train_X.shape[0], look_back, n_features))\n",
    "        test_X = test_X.reshape((test_X.shape[0], look_back, n_features))\n",
    "        #print(train_X.shape, train_y.shape, test_X.shape, test_y.shape)\n",
    "        # design network\n",
    "        model = Sequential()\n",
    "        model.add(LSTM(200, input_shape=(train_X.shape[1], train_X.shape[2]),return_sequences=True))\n",
    "        model.add(LSTM(200))\n",
    "        model.add(Dense(1))\n",
    "        model.compile(loss='mae', optimizer='Adamax')\n",
    "        model.summary()\n",
    "        # fit network\n",
    "        history = model.fit(train_X, train_y, epochs=25, batch_size=100, validation_data=(test_X, test_y), verbose=2, shuffle=False)\n",
    "        # plot history\n",
    "        plt.plot(history.history['loss'], label='train')\n",
    "        plt.plot(history.history['val_loss'], label='test')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "        figname=\"Plot_LSTM\"+str(i)+\".png\"\n",
    "        plt.savefig(figname,formatt=\"png\")\n",
    "        \n",
    "        # make a prediction of train set\n",
    "        ytrainpredict = model.predict(train_X)\n",
    "        train_X = train_X.reshape((train_X.shape[0], look_back*n_features))\n",
    "        inv_ytrainpredict = concatenate((ytrainpredict, train_X[:, -(n_features-1):]), axis=1)\n",
    "        inv_ytrainpredict = scaler.inverse_transform(inv_ytrainpredict)\n",
    "        inv_ytrainpredict = inv_ytrainpredict[:,0]\n",
    "        train_y = train_y.reshape((len(train_y), 1))\n",
    "        inv_trainy = concatenate((train_y, train_X[:, -(n_features-1):]), axis=1)\n",
    "        inv_trainy = scaler.inverse_transform(inv_trainy)\n",
    "        inv_trainy = inv_trainy[:,0]\n",
    "        #calculate score\n",
    "        trainMSE = mean_squared_error(inv_trainy, inv_ytrainpredict)\n",
    "        btrainMSE.append(trainMSE)\n",
    "        trainRMSE = math.sqrt(trainMSE)\n",
    "        btrainRMSE.append(trainRMSE)\n",
    "        train_MAPE = mean_absolute_error(inv_trainy, inv_ytrainpredict)/ (sum(inv_trainy)/len(inv_trainy)) * 100\n",
    "        btrainMAPE.append(train_MAPE)\n",
    "        train_SMAPE = smape(inv_trainy, inv_ytrainpredict)\n",
    "        btrainSMAPE.append(train_SMAPE)        \n",
    "        print('Train MSE: %.3f' %trainMSE)\n",
    "        print('Train RMSE: %.3f' %trainRMSE)\n",
    "        print('Train MAPE: %.3f' %train_MAPE)\n",
    "        print('Train SMAPE: %.3f' %train_SMAPE)\n",
    "        \n",
    "        \n",
    "        # make a prediction of test set\n",
    "        ytestpredtict = model.predict(test_X)        \n",
    "        test_X = test_X.reshape((test_X.shape[0], look_back*n_features))\n",
    "        # invert scaling for forecast\n",
    "        inv_ytestpredict = concatenate((ytestpredtict, test_X[:, -(n_features-1):]), axis=1)\n",
    "        inv_ytestpredict = scaler.inverse_transform(inv_ytestpredict)\n",
    "        inv_ytestpredict = inv_ytestpredict[:,0]\n",
    "        # invert scaling for actual\n",
    "        test_y = test_y.reshape((len(test_y), 1))\n",
    "        inv_y = concatenate((test_y, test_X[:, -(n_features-1):]), axis=1)\n",
    "        inv_y = scaler.inverse_transform(inv_y)\n",
    "        inv_y = inv_y[:,0]\n",
    "        # calculate score\n",
    "        testMSE = mean_squared_error(inv_y, inv_ytestpredict)\n",
    "        btestMSE.append(testMSE)\n",
    "        testRMSE = math.sqrt(testMSE)\n",
    "        btestRMSE.append(testRMSE)\n",
    "        test_MAPE = mean_absolute_error(inv_y, inv_ytestpredict)/ (sum(inv_y)/len(inv_y)) * 100\n",
    "        btestMAPE.append(test_MAPE)\n",
    "        test_SMAPE = smape(inv_y, inv_ytestpredict)\n",
    "        btestSMAPE.append(test_SMAPE)    \n",
    "        #rmse = math.sqrt(mean_squared_error(inv_y, inv_yhat))\n",
    "        print('Test MSE: %.3f' % testMSE)\n",
    "        print('Test RMSE: %.3f' % testRMSE)  \n",
    "        print('Test MAPE: %.3f' % test_MAPE)\n",
    "        print('Test SMAPE: %.3f' % test_SMAPE) \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Print results, \n",
    "print(\"Average train MSE = \", np.mean(btrainMSE))\n",
    "print(\"Average test MSE = \", np.mean(btestMSE))\n",
    "print(\"Average train MAPE = \", np.mean(btrainMAPE))\n",
    "print(\"Average test MAPE = \", np.mean(btestMAPE))\n",
    "print(\"Average train SMAPE = \", np.mean(btrainSMAPE))\n",
    "print(\"Average test SMAPE = \", np.mean(btestSMAPE))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM with kNN "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ctrainMSE=[]\n",
    "ctestMSE=[]\n",
    "ctrainRMSE=[]\n",
    "ctestRMSE=[]\n",
    "ctrainMAPE=[]\n",
    "ctestMAPE=[]\n",
    "ctrainSMAPE=[]\n",
    "ctestSMAPE=[]\n",
    "\n",
    "numberOfClusters = 63\n",
    "k = 15 #number of neighbours\n",
    "for i in range(numberOfClusters):    \n",
    "        print(\"Stand: %d\" %i)\n",
    "    #if (i in count_points_new):        \n",
    "        #print(\"Number of points: %d\" %count_points[i])\n",
    "        #Dataset for LSTM\n",
    "        dt=np.array([[j] for j in regionWisePickup_porto_data[i]])\n",
    "        #Take neighbour\n",
    "        for h in range(k):\n",
    "            dt1=np.array([[j] for j in regionWisePickup_porto_data[k_neighbour[i][h+1]]])\n",
    "            dt=np.concatenate((dt,dt1),axis=1)\n",
    "        #Generate a time series with count of POI for stand\n",
    "        #standpoi=np.full((len(regionWisePickup_porto_data[i]), 1), poi[i])\n",
    "        #dt=np.concatenate((dt,standpoi),axis=1)\n",
    "        # ensure all data is float\n",
    "        values = dt.astype('float32')\n",
    "        # normalize features\n",
    "        scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "        scaled = scaler.fit_transform(values)\n",
    "        look_back = 5 #number of data look back - to predict, default 1\n",
    "        # frame as supervised learning\n",
    "        reframed = series_to_supervised(scaled, look_back, 1)\n",
    "        # split into train and test sets\n",
    "        values = reframed.values\n",
    "        train_size = int(len(values) * 0.7)\n",
    "        test_size = len(values) - train_size\n",
    "        #n_train_hours\n",
    "        train = values[:train_size, :]\n",
    "        test = values[train_size:, :]\n",
    "        # split into input and outputs\n",
    "        #n_features = k + 2 # include itself and one extended feature\n",
    "        n_features = k + 1\n",
    "        n_obs = look_back * n_features\n",
    "        train_X, train_y = train[:, :n_obs], train[:, -n_features]\n",
    "        test_X, test_y = test[:, :n_obs], test[:, -n_features]\n",
    "        #print(train_X.shape, len(train_X), train_y.shape)\n",
    "        # reshape input to be 3D [samples, timesteps, features]\n",
    "        train_X = train_X.reshape((train_X.shape[0], look_back, n_features))\n",
    "        test_X = test_X.reshape((test_X.shape[0], look_back, n_features))\n",
    "        #print(train_X.shape, train_y.shape, test_X.shape, test_y.shape)\n",
    "        # design network\n",
    "        model = Sequential()\n",
    "        model.add(LSTM(200, input_shape=(train_X.shape[1], train_X.shape[2]),return_sequences=True))\n",
    "        model.add(LSTM(200))\n",
    "        model.add(Dense(1))\n",
    "        model.compile(loss='mae', optimizer='Adamax')\n",
    "        model.summary()\n",
    "        # fit network\n",
    "        history = model.fit(train_X, train_y, epochs=25, batch_size=100, validation_data=(test_X, test_y), verbose=2, shuffle=False)\n",
    "        # plot history\n",
    "        plt.plot(history.history['loss'], label='train')\n",
    "        plt.plot(history.history['val_loss'], label='test')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "        figname=\"Plot_LSTM_kNN_Test\"+str(i)+\".png\"\n",
    "        plt.savefig(figname,formatt=\"png\")\n",
    "        \n",
    "        # make a prediction of train set\n",
    "        ytrainpredict = model.predict(train_X)\n",
    "        train_X = train_X.reshape((train_X.shape[0], look_back*n_features))\n",
    "        inv_ytrainpredict = concatenate((ytrainpredict, train_X[:, -(n_features-1):]), axis=1)\n",
    "        inv_ytrainpredict = scaler.inverse_transform(inv_ytrainpredict)\n",
    "        inv_ytrainpredict = inv_ytrainpredict[:,0]\n",
    "        train_y = train_y.reshape((len(train_y), 1))\n",
    "        inv_trainy = concatenate((train_y, train_X[:, -(n_features-1):]), axis=1)\n",
    "        inv_trainy = scaler.inverse_transform(inv_trainy)\n",
    "        inv_trainy = inv_trainy[:,0]\n",
    "        #calculate score\n",
    "        trainMSE = mean_squared_error(inv_trainy, inv_ytrainpredict)\n",
    "        ctrainMSE.append(trainMSE)\n",
    "        trainRMSE = math.sqrt(trainMSE)\n",
    "        ctrainRMSE.append(trainRMSE)\n",
    "        train_MAPE = mean_absolute_error(inv_trainy, inv_ytrainpredict)/ (sum(inv_trainy)/len(inv_trainy)) * 100\n",
    "        ctrainMAPE.append(train_MAPE)\n",
    "        train_SMAPE = smape(inv_trainy, inv_ytrainpredict)\n",
    "        ctrainSMAPE.append(train_SMAPE)        \n",
    "        print('Train MSE: %.3f' %trainMSE)\n",
    "        print('Train RMSE: %.3f' %trainRMSE)\n",
    "        print('Train MAPE: %.3f' %train_MAPE)\n",
    "        print('Train SMAPE: %.3f' %train_SMAPE)\n",
    "        \n",
    "        \n",
    "        # make a prediction of test set\n",
    "        ytestpredtict = model.predict(test_X)        \n",
    "        test_X = test_X.reshape((test_X.shape[0], look_back*n_features))\n",
    "        # invert scaling for forecast\n",
    "        inv_ytestpredict = concatenate((ytestpredtict, test_X[:, -(n_features-1):]), axis=1)\n",
    "        inv_ytestpredict = scaler.inverse_transform(inv_ytestpredict)\n",
    "        inv_ytestpredict = inv_ytestpredict[:,0]\n",
    "        # invert scaling for actual\n",
    "        test_y = test_y.reshape((len(test_y), 1))\n",
    "        inv_y = concatenate((test_y, test_X[:, -(n_features-1):]), axis=1)\n",
    "        inv_y = scaler.inverse_transform(inv_y)\n",
    "        inv_y = inv_y[:,0]\n",
    "        # calculate score\n",
    "        testMSE = mean_squared_error(inv_y, inv_ytestpredict)\n",
    "        ctestMSE.append(testMSE)\n",
    "        testRMSE = math.sqrt(testMSE)\n",
    "        ctestRMSE.append(testRMSE)\n",
    "        test_MAPE = mean_absolute_error(inv_y, inv_ytestpredict)/ (sum(inv_y)/len(inv_y)) * 100\n",
    "        ctestMAPE.append(test_MAPE)\n",
    "        test_SMAPE = smape(inv_y, inv_ytestpredict)\n",
    "        ctestSMAPE.append(test_SMAPE)    \n",
    "        #rmse = math.sqrt(mean_squared_error(inv_y, inv_yhat))\n",
    "        print('Test MSE: %.3f' % testMSE)\n",
    "        print('Test RMSE: %.3f' % testRMSE)  \n",
    "        print('Test MAPE: %.3f' % test_MAPE)\n",
    "        print('Test SMAPE: %.3f' % test_SMAPE) \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Print results, \n",
    "print(\"Average train MSE = \", np.mean(ctrainMSE))\n",
    "print(\"Average test MSE = \", np.mean(ctestMSE))\n",
    "print(\"Average train MAPE = \", np.mean(ctrainMAPE))\n",
    "print(\"Average test MAPE = \", np.mean(ctestMAPE))\n",
    "print(\"Average train SMAPE = \", np.mean(ctrainSMAPE))\n",
    "print(\"Average test SMAPE = \", np.mean(ctestSMAPE))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Affect of K-neighbor "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ctrainMSE=[]\n",
    "ctestMSE=[]\n",
    "ctrainRMSE=[]\n",
    "ctestRMSE=[]\n",
    "ctrainMAPE=[]\n",
    "ctestMAPE=[]\n",
    "ctrainSMAPE=[]\n",
    "ctestSMAPE=[]\n",
    "\n",
    "numberOfClusters = 63\n",
    "#k = 15 #number of neighbours\n",
    "#Test for stand 11: regionWisePickup_porto_data[10]\n",
    "k=1\n",
    "while k<numberOfClusters-1:    \n",
    "        print(\"K= %d\" %k)\n",
    "    #if (i in count_points_new):        \n",
    "        #print(\"Number of points: %d\" %count_points[i])\n",
    "        #Dataset for LSTM\n",
    "        dt=np.array([[j] for j in regionWisePickup_porto_data[10]])\n",
    "        #Take neighbour\n",
    "        for h in range(k):\n",
    "            dt1=np.array([[j] for j in regionWisePickup_porto_data[k_neighbour[10][h+1]]])\n",
    "            dt=np.concatenate((dt,dt1),axis=1)\n",
    "        #Generate a time series with count of POI for stand\n",
    "        #standpoi=np.full((len(regionWisePickup_porto_data[i]), 1), poi[i])\n",
    "        #dt=np.concatenate((dt,standpoi),axis=1)\n",
    "        # ensure all data is float\n",
    "        values = dt.astype('float32')\n",
    "        # normalize features\n",
    "        scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "        scaled = scaler.fit_transform(values)\n",
    "        look_back = 5 #number of data look back - to predict, default 1\n",
    "        # frame as supervised learning\n",
    "        reframed = series_to_supervised(scaled, look_back, 1)\n",
    "        # split into train and test sets\n",
    "        values = reframed.values\n",
    "        train_size = int(len(values) * 0.7)\n",
    "        test_size = len(values) - train_size\n",
    "        #n_train_hours\n",
    "        train = values[:train_size, :]\n",
    "        test = values[train_size:, :]\n",
    "        # split into input and outputs\n",
    "        #n_features = k + 2 # include itself and one extended feature\n",
    "        n_features = k + 1\n",
    "        n_obs = look_back * n_features\n",
    "        train_X, train_y = train[:, :n_obs], train[:, -n_features]\n",
    "        test_X, test_y = test[:, :n_obs], test[:, -n_features]\n",
    "        #print(train_X.shape, len(train_X), train_y.shape)\n",
    "        # reshape input to be 3D [samples, timesteps, features]\n",
    "        train_X = train_X.reshape((train_X.shape[0], look_back, n_features))\n",
    "        test_X = test_X.reshape((test_X.shape[0], look_back, n_features))\n",
    "        #print(train_X.shape, train_y.shape, test_X.shape, test_y.shape)\n",
    "        # design network\n",
    "        model = Sequential()\n",
    "        model.add(LSTM(200, input_shape=(train_X.shape[1], train_X.shape[2]),return_sequences=True))\n",
    "        model.add(LSTM(200))\n",
    "        model.add(Dense(1))\n",
    "        model.compile(loss='mae', optimizer='Adamax')\n",
    "        model.summary()\n",
    "        # fit network\n",
    "        history = model.fit(train_X, train_y, epochs=10, batch_size=100, validation_data=(test_X, test_y), verbose=2, shuffle=False)\n",
    "        # plot history\n",
    "        plt.plot(history.history['loss'], label='train')\n",
    "        plt.plot(history.history['val_loss'], label='test')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "        figname=\"Plot_LSTM_kNN_Test\"+str(i)+\".png\"\n",
    "        plt.savefig(figname,formatt=\"png\")\n",
    "        \n",
    "        # make a prediction of train set\n",
    "        ytrainpredict = model.predict(train_X)\n",
    "        train_X = train_X.reshape((train_X.shape[0], look_back*n_features))\n",
    "        inv_ytrainpredict = concatenate((ytrainpredict, train_X[:, -(n_features-1):]), axis=1)\n",
    "        inv_ytrainpredict = scaler.inverse_transform(inv_ytrainpredict)\n",
    "        inv_ytrainpredict = inv_ytrainpredict[:,0]\n",
    "        train_y = train_y.reshape((len(train_y), 1))\n",
    "        inv_trainy = concatenate((train_y, train_X[:, -(n_features-1):]), axis=1)\n",
    "        inv_trainy = scaler.inverse_transform(inv_trainy)\n",
    "        inv_trainy = inv_trainy[:,0]\n",
    "        #calculate score\n",
    "        trainMSE = mean_squared_error(inv_trainy, inv_ytrainpredict)\n",
    "        ctrainMSE.append(trainMSE)\n",
    "        trainRMSE = math.sqrt(trainMSE)\n",
    "        ctrainRMSE.append(trainRMSE)\n",
    "        train_MAPE = mean_absolute_error(inv_trainy, inv_ytrainpredict)/ (sum(inv_trainy)/len(inv_trainy)) * 100\n",
    "        ctrainMAPE.append(train_MAPE)\n",
    "        train_SMAPE = smape(inv_trainy, inv_ytrainpredict)\n",
    "        ctrainSMAPE.append(train_SMAPE)        \n",
    "        print('Train MSE: %.3f' %trainMSE)\n",
    "        print('Train RMSE: %.3f' %trainRMSE)\n",
    "        print('Train MAPE: %.3f' %train_MAPE)\n",
    "        print('Train SMAPE: %.3f' %train_SMAPE)\n",
    "        \n",
    "        \n",
    "        # make a prediction of test set\n",
    "        ytestpredtict = model.predict(test_X)        \n",
    "        test_X = test_X.reshape((test_X.shape[0], look_back*n_features))\n",
    "        # invert scaling for forecast\n",
    "        inv_ytestpredict = concatenate((ytestpredtict, test_X[:, -(n_features-1):]), axis=1)\n",
    "        inv_ytestpredict = scaler.inverse_transform(inv_ytestpredict)\n",
    "        inv_ytestpredict = inv_ytestpredict[:,0]\n",
    "        # invert scaling for actual\n",
    "        test_y = test_y.reshape((len(test_y), 1))\n",
    "        inv_y = concatenate((test_y, test_X[:, -(n_features-1):]), axis=1)\n",
    "        inv_y = scaler.inverse_transform(inv_y)\n",
    "        inv_y = inv_y[:,0]\n",
    "        # calculate score\n",
    "        testMSE = mean_squared_error(inv_y, inv_ytestpredict)\n",
    "        ctestMSE.append(testMSE)\n",
    "        testRMSE = math.sqrt(testMSE)\n",
    "        ctestRMSE.append(testRMSE)\n",
    "        test_MAPE = mean_absolute_error(inv_y, inv_ytestpredict)/ (sum(inv_y)/len(inv_y)) * 100\n",
    "        ctestMAPE.append(test_MAPE)\n",
    "        test_SMAPE = smape(inv_y, inv_ytestpredict)\n",
    "        ctestSMAPE.append(test_SMAPE)    \n",
    "        #rmse = math.sqrt(mean_squared_error(inv_y, inv_yhat))\n",
    "        print('Test MSE: %.3f' % testMSE)\n",
    "        print('Test RMSE: %.3f' % testRMSE)  \n",
    "        print('Test MAPE: %.3f' % test_MAPE)\n",
    "        print('Test SMAPE: %.3f' % test_SMAPE)\n",
    "        \n",
    "        k=k+2"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
